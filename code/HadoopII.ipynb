{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbl+sjKefWv8++LBvzfDfp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adigenova/uohpmd/blob/main/code/HadoopII.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Java 8\n",
        "Hadoop es un framework de procesamiento de datos basado en java."
      ],
      "metadata": {
        "id": "khCyTGqeEj-0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntUhLC8XEcCN",
        "outputId": "0fd7a0e4-ad9b-43be-d660-4a5e12674ccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.16\" 2022-07-19\n",
            "OpenJDK Runtime Environment (build 11.0.16+8-post-Ubuntu-0ubuntu118.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.16+8-post-Ubuntu-0ubuntu118.04, mixed mode, sharing)\n"
          ]
        }
      ],
      "source": [
        "!java -version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5PpGZooE786",
        "outputId": "17a78e7a-fe0b-4be6-c00e-759389d30056"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 2 newly installed, 0 to remove and 12 not upgraded.\n",
            "Need to get 36.6 MB of archives.\n",
            "After this operation, 143 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jre-headless amd64 8u342-b07-0ubuntu1~18.04 [28.3 MB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jdk-headless amd64 8u342-b07-0ubuntu1~18.04 [8,300 kB]\n",
            "Fetched 36.6 MB in 2s (19.1 MB/s)\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "(Reading database ... 123934 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u342-b07-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u342-b07-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cambiamos la version de java pro defecto (elegir 2)\n",
        "!update-alternatives --config java"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6M4pBv9FPRW",
        "outputId": "0bde8cde-cfbb-4838-b35d-9ebff561d989"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative java (providing /usr/bin/java).\n",
            "\n",
            "  Selection    Path                                            Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java   1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!update-alternatives --config javac"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVqlFyjCFdDZ",
        "outputId": "214dbe14-f499-4254-8351-1e15ba91daae"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative javac (providing /usr/bin/javac).\n",
            "\n",
            "  Selection    Path                                          Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/javac   1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/javac   1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/bin/javac    1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!update-alternatives --config jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTzRtnRcFjkl",
        "outputId": "d0a2235a-1e03-4d4c-c988-ca632daa8a0d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative jps (providing /usr/bin/jps).\n",
            "\n",
            "  Selection    Path                                        Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/jps   1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/jps   1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/bin/jps    1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3NMuLCuFqaF",
        "outputId": "d6dc2e54-0d6c-4f4a-b195-b4a172369244"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"1.8.0_342\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_342-8u342-b07-0ubuntu1~18.04-b07)\n",
            "OpenJDK 64-Bit Server VM (build 25.342-b07, mixed mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creación de variables de entorno relacionadas con Java\n",
        "\n",
        "JAVA_HOME es una variable de entorno del sistema operativo que apunta a la ubicación del sistema de archivos donde se instaló JDK o JRE."
      ],
      "metadata": {
        "id": "mnELy7FzF32P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#en contrando el path de java\n",
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRjK3MR7FxQG",
        "outputId": "26f8b953-b31b-43e7-cef8-44dcb2e5bd09"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-8-openjdk-amd64/jre/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importando el module os\n",
        "import os\n",
        "#creando las variables de ambiente\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"JRE_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/jre\"\n",
        "os.environ[\"PATH\"] += \":$JAVA_HOME/bin:$JRE_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\""
      ],
      "metadata": {
        "id": "7xOy0zg7F9hv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalando SSH\n",
        "\n"
      ],
      "metadata": {
        "id": "pXhaEYZ4Kwun"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necesitamos definir un medio para que el nodo maestro acceda de forma remota a todos los nodos de nuestro clúster.\n",
        "\n",
        "Hadoop utiliza frases de contraseña SSH para la comunicación entre los nodos.\n",
        "\n",
        "SSH es un protocolo de red criptográfico para operar servicios de red de forma segura en una red no segura.\n",
        "\n",
        "SSH utiliza criptografía de clave pública estándar para crear un par de claves para la verificación del usuario: una pública y otra privada"
      ],
      "metadata": {
        "id": "PRke9v59K8hV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openssh-server"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l60zLteZKzFM",
        "outputId": "d6db0b98-3808-43be-aa09-180d77daba49"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  ncurses-term openssh-sftp-server python3-certifi python3-chardet\n",
            "  python3-idna python3-pkg-resources python3-requests python3-six\n",
            "  python3-urllib3 ssh-import-id\n",
            "Suggested packages:\n",
            "  molly-guard monkeysphere rssh ssh-askpass ufw python3-setuptools\n",
            "  python3-cryptography python3-openssl python3-socks\n",
            "The following NEW packages will be installed:\n",
            "  ncurses-term openssh-server openssh-sftp-server python3-certifi\n",
            "  python3-chardet python3-idna python3-pkg-resources python3-requests\n",
            "  python3-six python3-urllib3 ssh-import-id\n",
            "0 upgraded, 11 newly installed, 0 to remove and 12 not upgraded.\n",
            "Need to get 1,148 kB of archives.\n",
            "After this operation, 7,609 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 ncurses-term all 6.1-1ubuntu1.18.04 [248 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 openssh-sftp-server amd64 1:7.6p1-4ubuntu0.7 [45.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 openssh-server amd64 1:7.6p1-4ubuntu0.7 [332 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-certifi all 2018.1.18-2 [144 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-pkg-resources all 39.0.1-2 [98.8 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-chardet all 3.0.4-1 [80.3 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-idna all 2.6-1 [32.5 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-six all 1.11.0-2 [11.4 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-urllib3 all 1.22-1ubuntu0.18.04.2 [86.2 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-requests all 2.18.4-2ubuntu0.1 [58.3 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 ssh-import-id all 5.7-0ubuntu1.1 [10.9 kB]\n",
            "Fetched 1,148 kB in 0s (10.0 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package ncurses-term.\n",
            "(Reading database ... 124279 files and directories currently installed.)\n",
            "Preparing to unpack .../00-ncurses-term_6.1-1ubuntu1.18.04_all.deb ...\n",
            "Unpacking ncurses-term (6.1-1ubuntu1.18.04) ...\n",
            "Selecting previously unselected package openssh-sftp-server.\n",
            "Preparing to unpack .../01-openssh-sftp-server_1%3a7.6p1-4ubuntu0.7_amd64.deb ...\n",
            "Unpacking openssh-sftp-server (1:7.6p1-4ubuntu0.7) ...\n",
            "Selecting previously unselected package openssh-server.\n",
            "Preparing to unpack .../02-openssh-server_1%3a7.6p1-4ubuntu0.7_amd64.deb ...\n",
            "Unpacking openssh-server (1:7.6p1-4ubuntu0.7) ...\n",
            "Selecting previously unselected package python3-certifi.\n",
            "Preparing to unpack .../03-python3-certifi_2018.1.18-2_all.deb ...\n",
            "Unpacking python3-certifi (2018.1.18-2) ...\n",
            "Selecting previously unselected package python3-pkg-resources.\n",
            "Preparing to unpack .../04-python3-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python3-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python3-chardet.\n",
            "Preparing to unpack .../05-python3-chardet_3.0.4-1_all.deb ...\n",
            "Unpacking python3-chardet (3.0.4-1) ...\n",
            "Selecting previously unselected package python3-idna.\n",
            "Preparing to unpack .../06-python3-idna_2.6-1_all.deb ...\n",
            "Unpacking python3-idna (2.6-1) ...\n",
            "Selecting previously unselected package python3-six.\n",
            "Preparing to unpack .../07-python3-six_1.11.0-2_all.deb ...\n",
            "Unpacking python3-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python3-urllib3.\n",
            "Preparing to unpack .../08-python3-urllib3_1.22-1ubuntu0.18.04.2_all.deb ...\n",
            "Unpacking python3-urllib3 (1.22-1ubuntu0.18.04.2) ...\n",
            "Selecting previously unselected package python3-requests.\n",
            "Preparing to unpack .../09-python3-requests_2.18.4-2ubuntu0.1_all.deb ...\n",
            "Unpacking python3-requests (2.18.4-2ubuntu0.1) ...\n",
            "Selecting previously unselected package ssh-import-id.\n",
            "Preparing to unpack .../10-ssh-import-id_5.7-0ubuntu1.1_all.deb ...\n",
            "Unpacking ssh-import-id (5.7-0ubuntu1.1) ...\n",
            "Setting up ncurses-term (6.1-1ubuntu1.18.04) ...\n",
            "Setting up python3-idna (2.6-1) ...\n",
            "Setting up python3-six (1.11.0-2) ...\n",
            "Setting up python3-certifi (2018.1.18-2) ...\n",
            "Setting up python3-pkg-resources (39.0.1-2) ...\n",
            "Setting up openssh-sftp-server (1:7.6p1-4ubuntu0.7) ...\n",
            "Setting up python3-chardet (3.0.4-1) ...\n",
            "Setting up python3-urllib3 (1.22-1ubuntu0.18.04.2) ...\n",
            "Setting up openssh-server (1:7.6p1-4ubuntu0.7) ...\n",
            "\n",
            "Creating config file /etc/ssh/sshd_config with new version\n",
            "Creating SSH2 RSA key; this may take some time ...\n",
            "2048 SHA256:ra58fn/YN8sd3LCPY0X3woMz7B7s65Tm8Ja+YQ9JXsM root@7c99dff363e7 (RSA)\n",
            "Creating SSH2 ECDSA key; this may take some time ...\n",
            "256 SHA256:E9VgxFizy0Td8lMyUS7adHlkJoKoBUaHrg0tYp1hBJc root@7c99dff363e7 (ECDSA)\n",
            "Creating SSH2 ED25519 key; this may take some time ...\n",
            "256 SHA256:zoTfQYhJpPH3vGudf3e+bvJuQB9S/W84EmDYYs3JIYw root@7c99dff363e7 (ED25519)\n",
            "Created symlink /etc/systemd/system/sshd.service → /lib/systemd/system/ssh.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/ssh.service → /lib/systemd/system/ssh.service.\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up python3-requests (2.18.4-2ubuntu0.1) ...\n",
            "Setting up ssh-import-id (5.7-0ubuntu1.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for systemd (237-3ubuntu10.56) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Starting the server\n",
        "!service ssh start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7EDXyoSLAgi",
        "outputId": "2cbec572-d369-4001-c3a4-a335abfe8701"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Starting OpenBSD Secure Shell server sshd\n",
            "   ...done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep Port /etc/ssh/sshd_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNKXUeCzLFqO",
        "outputId": "8c5b2fc3-bdd2-4e08-cb2d-5205488f5647"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Port 22\n",
            "#GatewayPorts no\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzq6op9-LQRq",
        "outputId": "97b8827c-76bb-4971-cbe4-6143deaabcd1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating public/private rsa key pair.\n",
            "Created directory '/root/.ssh'.\n",
            "Your identification has been saved in /root/.ssh/id_rsa.\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub.\n",
            "The key fingerprint is:\n",
            "SHA256:ObfHzFg16eFouK93+hyn9tjaB3rP6HuICSpy4Rwy7yk root@7c99dff363e7\n",
            "The key's randomart image is:\n",
            "+---[RSA 2048]----+\n",
            "|                 |\n",
            "|               . |\n",
            "|              =  |\n",
            "|         . . = o |\n",
            "|        S o + o  |\n",
            "|    o o  o.X  .  |\n",
            "|     * o .+.=+.o.|\n",
            "|    E *..  o=.=Xo|\n",
            "|     =o.  .ooBX*B|\n",
            "+----[SHA256]-----+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Copying the key to autorized keys\n",
        "!cat $HOME/.ssh/id_rsa.pub>>$HOME/.ssh/authorized_keys\n",
        "#Changing the permissions on the key\n",
        "!chmod 0600 ~/.ssh/authorized_keys"
      ],
      "metadata": {
        "id": "N0e-UAsPLUZt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Conneting with the local machine\n",
        "!ssh -o StrictHostKeyChecking=no localhost uptime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhI5w6TGLXpR",
        "outputId": "5884d206-d37b-4cc4-b38d-60b17962fde0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.\r\n",
            " 18:44:15 up 2 min,  0 users,  load average: 0.38, 0.27, 0.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Hadoop 3.2.3"
      ],
      "metadata": {
        "id": "Xo_paSdCLc7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget  https://archive.apache.org/dist/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB_eAifNLeuH",
        "outputId": "d5f8f2a3-f26e-4da4-ac7f-0d029259ac7a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-06 18:44:20--  https://archive.apache.org/dist/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz\n",
            "Resolving archive.apache.org (archive.apache.org)... 138.201.131.134, 2a01:4f8:172:2ec5::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|138.201.131.134|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 492241961 (469M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.2.3.tar.gz’\n",
            "\n",
            "hadoop-3.2.3.tar.gz 100%[===================>] 469.44M  43.2MB/s    in 8.6s    \n",
            "\n",
            "2022-10-06 18:44:29 (54.5 MB/s) - ‘hadoop-3.2.3.tar.gz’ saved [492241961/492241961]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo tar -xzf hadoop-3.2.3.tar.gz\n",
        "!cp -r hadoop-3.2.3/ /usr/local/\n",
        "!ls /usr/local/hadoop-3.2.3/etc/hadoop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1BMqUQtMNxi",
        "outputId": "de201c3f-c526-4e8b-d700-aa22f9ec22d7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "capacity-scheduler.xml\t\t  kms-log4j.properties\n",
            "configuration.xsl\t\t  kms-site.xml\n",
            "container-executor.cfg\t\t  log4j.properties\n",
            "core-site.xml\t\t\t  mapred-env.cmd\n",
            "hadoop-env.cmd\t\t\t  mapred-env.sh\n",
            "hadoop-env.sh\t\t\t  mapred-queues.xml.template\n",
            "hadoop-metrics2.properties\t  mapred-site.xml\n",
            "hadoop-policy.xml\t\t  shellprofile.d\n",
            "hadoop-user-functions.sh.example  ssl-client.xml.example\n",
            "hdfs-site.xml\t\t\t  ssl-server.xml.example\n",
            "httpfs-env.sh\t\t\t  user_ec_policies.xml.template\n",
            "httpfs-log4j.properties\t\t  workers\n",
            "httpfs-signature.secret\t\t  yarn-env.cmd\n",
            "httpfs-site.xml\t\t\t  yarn-env.sh\n",
            "kms-acls.xml\t\t\t  yarnservice-log4j.properties\n",
            "kms-env.sh\t\t\t  yarn-site.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i '/export JAVA_HOME=/a export JAVA_HOME=\\/usr\\/lib\\/jvm\\/java-8-openjdk-amd64' /usr/local/hadoop-3.2.3/etc/hadoop/hadoop-env.sh"
      ],
      "metadata": {
        "id": "JTMXlQv_Mhc7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.2.3\""
      ],
      "metadata": {
        "id": "7FE7MO-ZMpds"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls $HADOOP_HOME/etc/hadoop/*.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWlny-uxMvPc",
        "outputId": "cf8d99b9-77e4-496b-dc09-22eed16268fa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/hadoop-3.2.3/etc/hadoop/capacity-scheduler.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/core-site.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/hadoop-policy.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/hdfs-site.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/httpfs-site.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/kms-acls.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/kms-site.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/mapred-site.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/yarn-site.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat $HADOOP_HOME/etc/hadoop/core-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW_AfNd6M9X4",
        "outputId": "baada3e9-e833-48cf-aa72-2849a33e7f00"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "\n",
            "<!-- Put site-specific property overrides in this file. -->\n",
            "\n",
            "<configuration>\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Hadoop in standalone mode\n",
        "\n",
        "Con las propiedades de configuración predeterminadas, Hadoop se ejecuta en modo independiente (modo no distribuido). Es decir, el modo independiente (también conocido como modo local) es el modo predeterminado para Hadoop.\n",
        "\n",
        "No hay demonios para ejecutar. Solo un solo proceso java\n",
        "\n",
        "Se utilizan el sistema de archivos local y el ejecutor de trabajos MapReduce local\n",
        "\n",
        "El comando para ejecutar un programa Hadoop mapreduce que está escrito en Java es:\n",
        "\n",
        "```\n",
        "$HADOOP_HOME/bin/hadoop jar <jar>\n",
        "```"
      ],
      "metadata": {
        "id": "QNb_VOY3M-4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls $HADOOP_HOME/share/hadoop/mapreduce/*.jar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZf6kqikNUB2",
        "outputId": "70fa7643-8c97-4f63-e55f-56677e35091e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3-tests.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplos Map/reduce"
      ],
      "metadata": {
        "id": "wzpaLSzFNj3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VePKmgiINmPz",
        "outputId": "7191eedd-0ba9-413d-f833-1c2a1b3618e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An example program must be given as the first argument.\n",
            "Valid program names are:\n",
            "  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.\n",
            "  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.\n",
            "  bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.\n",
            "  dbcount: An example job that count the pageview counts from a database.\n",
            "  distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.\n",
            "  grep: A map/reduce program that counts the matches of a regex in the input.\n",
            "  join: A job that effects a join over sorted, equally partitioned datasets\n",
            "  multifilewc: A job that counts words from several files.\n",
            "  pentomino: A map/reduce tile laying program to find solutions to pentomino problems.\n",
            "  pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.\n",
            "  randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.\n",
            "  randomwriter: A map/reduce program that writes 10GB of random data per node.\n",
            "  secondarysort: An example defining a secondary sort to the reduce.\n",
            "  sort: A map/reduce program that sorts the data written by the random writer.\n",
            "  sudoku: A sudoku solver.\n",
            "  teragen: Generate data for the terasort\n",
            "  terasort: Run the terasort\n",
            "  teravalidate: Checking results of terasort\n",
            "  wordcount: A map/reduce program that counts the words in the input files.\n",
            "  wordmean: A map/reduce program that counts the average length of the words in the input files.\n",
            "  wordmedian: A map/reduce program that counts the median length of the words in the input files.\n",
            "  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar wordcount"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQGDQcsuNvu2",
        "outputId": "dd5ba883-3d18-46d5-dee8-842bd1a95ab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: wordcount <in> [<in>...] <out>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget  https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Cl1ICh5N3Vk",
        "outputId": "88de7f8d-c985-4a6f-e885-b7aceccf4dc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-03 14:20:13--  https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt\n",
            "Resolving www.mirrorservice.org (www.mirrorservice.org)... 212.219.56.184, 2001:630:341:12::184\n",
            "Connecting to www.mirrorservice.org (www.mirrorservice.org)|212.219.56.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 678064 (662K) [text/plain]\n",
            "Saving to: ‘101.txt’\n",
            "\n",
            "\r101.txt               0%[                    ]       0  --.-KB/s               \r101.txt             100%[===================>] 662.17K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-10-03 14:20:13 (10.3 MB/s) - ‘101.txt’ saved [678064/678064]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar \\\n",
        "wordcount /content/101.txt /content/output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPqP2imoOBAk",
        "outputId": "5064907e-b33b-46a0-c7c0-5afba41a07ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-10-03 14:21:09,489 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2022-10-03 14:21:09,594 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2022-10-03 14:21:09,594 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2022-10-03 14:21:09,793 INFO input.FileInputFormat: Total input files to process : 1\n",
            "2022-10-03 14:21:09,837 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2022-10-03 14:21:10,042 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local590016641_0001\n",
            "2022-10-03 14:21:10,042 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2022-10-03 14:21:10,241 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2022-10-03 14:21:10,242 INFO mapreduce.Job: Running job: job_local590016641_0001\n",
            "2022-10-03 14:21:10,249 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2022-10-03 14:21:10,258 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-10-03 14:21:10,258 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-10-03 14:21:10,258 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "2022-10-03 14:21:10,316 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2022-10-03 14:21:10,317 INFO mapred.LocalJobRunner: Starting task: attempt_local590016641_0001_m_000000_0\n",
            "2022-10-03 14:21:10,348 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-10-03 14:21:10,348 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-10-03 14:21:10,389 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-10-03 14:21:10,396 INFO mapred.MapTask: Processing split: file:/content/101.txt:0+678064\n",
            "2022-10-03 14:21:10,499 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2022-10-03 14:21:10,500 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2022-10-03 14:21:10,500 INFO mapred.MapTask: soft limit at 83886080\n",
            "2022-10-03 14:21:10,500 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2022-10-03 14:21:10,500 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2022-10-03 14:21:10,504 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2022-10-03 14:21:10,808 INFO mapred.LocalJobRunner: \n",
            "2022-10-03 14:21:10,808 INFO mapred.MapTask: Starting flush of map output\n",
            "2022-10-03 14:21:10,808 INFO mapred.MapTask: Spilling map output\n",
            "2022-10-03 14:21:10,809 INFO mapred.MapTask: bufstart = 0; bufend = 1078906; bufvoid = 104857600\n",
            "2022-10-03 14:21:10,809 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25792448(103169792); length = 421949/6553600\n",
            "2022-10-03 14:21:11,247 INFO mapreduce.Job: Job job_local590016641_0001 running in uber mode : false\n",
            "2022-10-03 14:21:11,249 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2022-10-03 14:21:11,643 INFO mapred.MapTask: Finished spill 0\n",
            "2022-10-03 14:21:11,662 INFO mapred.Task: Task:attempt_local590016641_0001_m_000000_0 is done. And is in the process of committing\n",
            "2022-10-03 14:21:11,667 INFO mapred.LocalJobRunner: map\n",
            "2022-10-03 14:21:11,667 INFO mapred.Task: Task 'attempt_local590016641_0001_m_000000_0' done.\n",
            "2022-10-03 14:21:11,681 INFO mapred.Task: Final Counters for attempt_local590016641_0001_m_000000_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=994673\n",
            "\t\tFILE: Number of bytes written=1178697\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=13006\n",
            "\t\tMap output records=105488\n",
            "\t\tMap output bytes=1078906\n",
            "\t\tMap output materialized bytes=318005\n",
            "\t\tInput split bytes=86\n",
            "\t\tCombine input records=105488\n",
            "\t\tCombine output records=21438\n",
            "\t\tSpilled Records=21438\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=258473984\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=678064\n",
            "2022-10-03 14:21:11,681 INFO mapred.LocalJobRunner: Finishing task: attempt_local590016641_0001_m_000000_0\n",
            "2022-10-03 14:21:11,682 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2022-10-03 14:21:11,687 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2022-10-03 14:21:11,688 INFO mapred.LocalJobRunner: Starting task: attempt_local590016641_0001_r_000000_0\n",
            "2022-10-03 14:21:11,700 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-10-03 14:21:11,700 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-10-03 14:21:11,701 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-10-03 14:21:11,706 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1be257e5\n",
            "2022-10-03 14:21:11,709 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-10-03 14:21:11,746 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2119434240, maxSingleShuffleLimit=529858560, mergeThreshold=1398826624, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2022-10-03 14:21:11,761 INFO reduce.EventFetcher: attempt_local590016641_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2022-10-03 14:21:11,818 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local590016641_0001_m_000000_0 decomp: 318001 len: 318005 to MEMORY\n",
            "2022-10-03 14:21:11,825 INFO reduce.InMemoryMapOutput: Read 318001 bytes from map-output for attempt_local590016641_0001_m_000000_0\n",
            "2022-10-03 14:21:11,827 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 318001, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->318001\n",
            "2022-10-03 14:21:11,830 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2022-10-03 14:21:11,831 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-10-03 14:21:11,832 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2022-10-03 14:21:11,844 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-10-03 14:21:11,845 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 317994 bytes\n",
            "2022-10-03 14:21:11,894 INFO reduce.MergeManagerImpl: Merged 1 segments, 318001 bytes to disk to satisfy reduce memory limit\n",
            "2022-10-03 14:21:11,896 INFO reduce.MergeManagerImpl: Merging 1 files, 318005 bytes from disk\n",
            "2022-10-03 14:21:11,899 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2022-10-03 14:21:11,899 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-10-03 14:21:11,901 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 317994 bytes\n",
            "2022-10-03 14:21:11,902 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-10-03 14:21:11,908 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2022-10-03 14:21:12,036 INFO mapred.Task: Task:attempt_local590016641_0001_r_000000_0 is done. And is in the process of committing\n",
            "2022-10-03 14:21:12,037 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-10-03 14:21:12,038 INFO mapred.Task: Task attempt_local590016641_0001_r_000000_0 is allowed to commit now\n",
            "2022-10-03 14:21:12,041 INFO output.FileOutputCommitter: Saved output of task 'attempt_local590016641_0001_r_000000_0' to file:/content/output\n",
            "2022-10-03 14:21:12,042 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2022-10-03 14:21:12,042 INFO mapred.Task: Task 'attempt_local590016641_0001_r_000000_0' done.\n",
            "2022-10-03 14:21:12,043 INFO mapred.Task: Final Counters for attempt_local590016641_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1630715\n",
            "\t\tFILE: Number of bytes written=1732174\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=21438\n",
            "\t\tReduce shuffle bytes=318005\n",
            "\t\tReduce input records=21438\n",
            "\t\tReduce output records=21438\n",
            "\t\tSpilled Records=21438\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=17\n",
            "\t\tTotal committed heap usage (bytes)=258473984\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=235472\n",
            "2022-10-03 14:21:12,043 INFO mapred.LocalJobRunner: Finishing task: attempt_local590016641_0001_r_000000_0\n",
            "2022-10-03 14:21:12,043 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2022-10-03 14:21:12,252 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2022-10-03 14:21:12,253 INFO mapreduce.Job: Job job_local590016641_0001 completed successfully\n",
            "2022-10-03 14:21:12,267 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=2625388\n",
            "\t\tFILE: Number of bytes written=2910871\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=13006\n",
            "\t\tMap output records=105488\n",
            "\t\tMap output bytes=1078906\n",
            "\t\tMap output materialized bytes=318005\n",
            "\t\tInput split bytes=86\n",
            "\t\tCombine input records=105488\n",
            "\t\tCombine output records=21438\n",
            "\t\tReduce input groups=21438\n",
            "\t\tReduce shuffle bytes=318005\n",
            "\t\tReduce input records=21438\n",
            "\t\tReduce output records=21438\n",
            "\t\tSpilled Records=42876\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=17\n",
            "\t\tTotal committed heap usage (bytes)=516947968\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=678064\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=235472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DZD89ZZORB1",
        "outputId": "cb7c5f30-bf1c-42fa-9319-c54be3958582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-r-00000  _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -50 /content/output/part-r-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMG4IkXROXIs",
        "outputId": "9e3a7e23-0d2e-4c86-db51-aaf056e54e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"(d)\t1\n",
            "\"/H\"\t1\n",
            "\"0\"\t1\n",
            "\"02\"\t2\n",
            "\"1.\t1\n",
            "\"414\t3\n",
            "\"A\t2\n",
            "\"AT&T\t3\n",
            "\"AT&T's\t1\n",
            "\"Access\t1\n",
            "\"Acid\t3\n",
            "\"Ad-hocracy\"\t1\n",
            "\"Advanced\t1\n",
            "\"Agents\t1\n",
            "\"Al\t3\n",
            "\"All\t3\n",
            "\"American\t1\n",
            "\"An\t1\n",
            "\"And\t2\n",
            "\"Any\t1\n",
            "\"Are\t3\n",
            "\"Artificial\t1\n",
            "\"As\t1\n",
            "\"Assistant\t1\n",
            "\"Attctc\"\t2\n",
            "\"Auld\t1\n",
            "\"Autodesk,\"\t1\n",
            "\"BBS,\"\t2\n",
            "\"BIRTHPLACE\t1\n",
            "\"BRITS\t1\n",
            "\"Barry\t1\n",
            "\"Because\t1\n",
            "\"Before\t1\n",
            "\"Bell\t2\n",
            "\"Bell\"\t1\n",
            "\"BellSouth\t2\n",
            "\"Berkeley\t1\n",
            "\"Big\t1\n",
            "\"Biggest\t1\n",
            "\"Black\t3\n",
            "\"Blue\t1\n",
            "\"Bob\"\t1\n",
            "\"Bob,\t3\n",
            "\"Bob:\t1\n",
            "\"Bullet-N-Board.\"\t1\n",
            "\"Bureaucrat-ese\t1\n",
            "\"But\t4\n",
            "\"C-word.\"\t1\n",
            "\"CALIFORNIA\"\t1\n",
            "\"CC,\"\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cat /content/output/part-r-00000 | sort -rn -k2,2 | head -n 20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7ONdp1LOckJ",
        "outputId": "079afdd5-c62c-4924-b13a-22350dea21f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the\t5100\n",
            "of\t3356\n",
            "and\t2765\n",
            "a\t2434\n",
            "to\t2243\n",
            "in\t1693\n",
            "was\t1096\n",
            "is\t829\n",
            "for\t781\n",
            "that\t771\n",
            "The\t751\n",
            "had\t719\n",
            "with\t641\n",
            "on\t594\n",
            "as\t572\n",
            "by\t539\n",
            "it\t504\n",
            "this\t489\n",
            "not\t486\n",
            "are\t477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Hadoop in Pseudo-distributed mode\n",
        "\n",
        "En el modo Pseudodistribuido entran en juego todos los componentes distribuidos de Hadoop. Es decir, todos los demonios de Hadoop responsables del almacenamiento distribuido y el procesamiento distribuido se ejecutarán en la misma máquina.\n",
        "\n",
        "Demonios maestros:\n",
        "\n",
        "1. NameNode\n",
        "2. Resource Manager\n",
        "3. Standby NameNode\n",
        "\n",
        "Demonios esclavos:\n",
        "\n",
        "1. DataNode\n",
        "2. Node Manager"
      ],
      "metadata": {
        "id": "JCwjdIOwPIqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding required property to core-site.xlm file\n",
        "!sed -i '/<configuration>/a\\\n",
        "  <property>\\n\\\n",
        "    <name>fs.defaultFS</name>\\n\\\n",
        "    <value>hdfs://localhost:9000</value>\\n\\\n",
        "  </property>' \\\n",
        "$HADOOP_HOME/etc/hadoop/core-site.xml\n"
      ],
      "metadata": {
        "id": "q0tS2ZyyRqnd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat $HADOOP_HOME/etc/hadoop/core-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnp90asyR3hi",
        "outputId": "2f4901e6-b16b-4253-b1d3-3c0773b92b82"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "\n",
            "<!-- Put site-specific property overrides in this file. -->\n",
            "\n",
            "<configuration>\n",
            "<property>\n",
            "     <name>fs.defaultFS</name>\n",
            "     <value>hdfs://localhost:9000</value>\n",
            "   </property>\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i '/<configuration>/a\\\n",
        "  <property>\\n\\\n",
        "    <name>dfs.replication</name>\\n\\\n",
        "    <value>1</value>\\n\\\n",
        "  </property>' \\\n",
        "$HADOOP_HOME/etc/hadoop/hdfs-site.xml"
      ],
      "metadata": {
        "id": "XegGEgBvTwwG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat $HADOOP_HOME/etc/hadoop/hdfs-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXmyyxRhTzZm",
        "outputId": "30fdbcaf-68e2-452e-a4b9-37e9d8579881"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "\n",
            "<!-- Put site-specific property overrides in this file. -->\n",
            "\n",
            "<configuration>\n",
            "<property>\n",
            "     <name>dfs.replication</name>\n",
            "     <value>1</value>\n",
            "   </property>\n",
            "\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding required properties to mapred-site.xml file\n",
        "!sed -i '/<configuration>/a\\\n",
        "  <property>\\n\\\n",
        "    <name>mapreduce.framework.name</name>\\n\\\n",
        "    <value>yarn</value>\\n\\\n",
        "  </property>\\n\\\n",
        "  <property>\\n\\\n",
        "    <name>mapreduce.application.classpath</name>\\n\\\n",
        "    <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>\\n\\\n",
        "  </property>' \\\n",
        "$HADOOP_HOME/etc/hadoop/mapred-site.xml"
      ],
      "metadata": {
        "id": "CfkkXPijR-NX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat $HADOOP_HOME/etc/hadoop/mapred-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNN17Kg8UA2j",
        "outputId": "6916eb4e-7d79-40e7-8174-7398c7546375"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "\n",
            "<!-- Put site-specific property overrides in this file. -->\n",
            "\n",
            "<configuration>\n",
            "<property>\n",
            "     <name>mapreduce.framework.name</name>\n",
            "     <value>yarn</value>\n",
            "   </property>\n",
            "   <property>\n",
            "     <name>mapreduce.application.classpath</name>\n",
            "     <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>\n",
            "   </property>\n",
            "\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding required properties to yarn-site.xml file\n",
        "!sed -i '/<configuration>/a\\\n",
        "  <property>\\n\\\n",
        "    <description>The hostname of the RM.</description>\\n\\\n",
        "    <name>yarn.resourcemanager.hostname</name>\\n\\\n",
        "    <value>localhost</value>\\n\\\n",
        "  </property>\\n\\\n",
        "  <property>\\n\\\n",
        "    <name>yarn.nodemanager.aux-services</name>\\n\\\n",
        "    <value>mapreduce_shuffle</value>\\n\\\n",
        "  </property>\\n\\\n",
        "  <property>\\n\\\n",
        "    <name>yarn.nodemanager.env-whitelist</name>\\n\\\n",
        "    <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME</value>\\n\\\n",
        "  </property>' \\\n",
        "$HADOOP_HOME/etc/hadoop/yarn-site.xml"
      ],
      "metadata": {
        "id": "LdGCJEBOUFwF"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Content of yarn-site.xml after the editing\n",
        "!cat $HADOOP_HOME/etc/hadoop/yarn-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rt8s4GxSR_Eh",
        "outputId": "2a7097d7-d0a1-4b92-cdf8-4e81cf94ecea"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "<configuration>\n",
            "<property>\n",
            "     <description>The hostname of the RM.</description>\n",
            "     <name>yarn.resourcemanager.hostname</name>\n",
            "     <value>localhost</value>\n",
            "   </property>\n",
            "   <property>\n",
            "     <name>yarn.nodemanager.aux-services</name>\n",
            "     <value>mapreduce_shuffle</value>\n",
            "   </property>\n",
            "   <property>\n",
            "     <name>yarn.nodemanager.env-whitelist</name>\n",
            "     <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME</value>\n",
            "   </property>\n",
            "\n",
            "<!-- Site specific YARN configuration properties -->\n",
            "\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Formateo del sistema de archivos HDFS\n",
        "\n",
        "Antes de poder utilizar HDFS por primera vez, se debe formatear el sistema de archivos. El proceso de formateo crea un sistema de archivos vacío al crear los directorios de almacenamiento y las versiones iniciales de los NameNodes."
      ],
      "metadata": {
        "id": "D-_T0kPiSPQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs namenode -format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wNQPizCSBk8",
        "outputId": "89b92432-c6c3-4312-de56-5bec120d3879"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: /usr/local/hadoop-3.2.3/logs does not exist. Creating.\n",
            "2022-10-06 18:45:55,413 INFO namenode.NameNode: STARTUP_MSG: \n",
            "/************************************************************\n",
            "STARTUP_MSG: Starting NameNode\n",
            "STARTUP_MSG:   host = 7c99dff363e7/172.28.0.2\n",
            "STARTUP_MSG:   args = [-format]\n",
            "STARTUP_MSG:   version = 3.2.3\n",
            "STARTUP_MSG:   classpath = /usr/local/hadoop-3.2.3/etc/hadoop:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/hadoop-auth-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/accessors-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-io-2.8.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/javax.activation-api-1.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-server-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-framework-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-client-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/hadoop-annotations-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/json-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-lang3-3.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-databind-2.10.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-webapp-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-compress-1.21.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/netty-3.10.6.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-text-1.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-ajax-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-io-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-servlet-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-codec-1.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/asm-5.0.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/woodstox-core-5.3.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/spotbugs-annotations-3.1.9.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-security-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-xml-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-net-3.6.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/zookeeper-3.4.14.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-http-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-nfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-kms-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-auth-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/accessors-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.activation-api-1.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-server-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-annotations-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/json-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-databind-2.10.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-webapp-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-text-1.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/okio-1.6.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-io-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-servlet-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-all-4.1.68.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/woodstox-core-5.3.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/spotbugs-annotations-3.1.9.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-security-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-xml-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-net-3.6.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/zookeeper-3.4.14.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-http-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/lib/junit-4.13.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/snakeyaml-1.26.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/objenesis-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/guice-4.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-submarine-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-router-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-core-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-registry-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-api-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-api-3.2.3.jar\n",
            "STARTUP_MSG:   build = https://github.com/apache/hadoop -r abe5358143720085498613d399be3bbf01e0f131; compiled by 'ubuntu' on 2022-03-20T01:18Z\n",
            "STARTUP_MSG:   java = 1.8.0_342\n",
            "************************************************************/\n",
            "2022-10-06 18:45:55,449 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
            "2022-10-06 18:45:55,557 INFO namenode.NameNode: createNameNode [-format]\n",
            "Formatting using clusterid: CID-9be25c73-e603-4e4f-8be8-cebf9bd7c359\n",
            "2022-10-06 18:45:56,123 INFO namenode.FSEditLog: Edit logging is async:true\n",
            "2022-10-06 18:45:56,153 INFO namenode.FSNamesystem: KeyProvider: null\n",
            "2022-10-06 18:45:56,155 INFO namenode.FSNamesystem: fsLock is fair: true\n",
            "2022-10-06 18:45:56,155 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
            "2022-10-06 18:45:56,161 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\n",
            "2022-10-06 18:45:56,161 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
            "2022-10-06 18:45:56,161 INFO namenode.FSNamesystem: isPermissionEnabled = true\n",
            "2022-10-06 18:45:56,161 INFO namenode.FSNamesystem: HA Enabled: false\n",
            "2022-10-06 18:45:56,221 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
            "2022-10-06 18:45:56,234 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n",
            "2022-10-06 18:45:56,234 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
            "2022-10-06 18:45:56,239 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
            "2022-10-06 18:45:56,240 INFO blockmanagement.BlockManager: The block deletion will start around 2022 Oct 06 18:45:56\n",
            "2022-10-06 18:45:56,241 INFO util.GSet: Computing capacity for map BlocksMap\n",
            "2022-10-06 18:45:56,241 INFO util.GSet: VM type       = 64-bit\n",
            "2022-10-06 18:45:56,242 INFO util.GSet: 2.0% max memory 2.8 GB = 57.8 MB\n",
            "2022-10-06 18:45:56,242 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n",
            "2022-10-06 18:45:56,253 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
            "2022-10-06 18:45:56,253 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
            "2022-10-06 18:45:56,259 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
            "2022-10-06 18:45:56,259 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
            "2022-10-06 18:45:56,259 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
            "2022-10-06 18:45:56,260 INFO blockmanagement.BlockManager: defaultReplication         = 1\n",
            "2022-10-06 18:45:56,260 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
            "2022-10-06 18:45:56,260 INFO blockmanagement.BlockManager: minReplication             = 1\n",
            "2022-10-06 18:45:56,260 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
            "2022-10-06 18:45:56,260 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
            "2022-10-06 18:45:56,260 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
            "2022-10-06 18:45:56,260 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
            "2022-10-06 18:45:56,279 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
            "2022-10-06 18:45:56,279 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
            "2022-10-06 18:45:56,279 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
            "2022-10-06 18:45:56,280 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
            "2022-10-06 18:45:56,291 INFO util.GSet: Computing capacity for map INodeMap\n",
            "2022-10-06 18:45:56,291 INFO util.GSet: VM type       = 64-bit\n",
            "2022-10-06 18:45:56,291 INFO util.GSet: 1.0% max memory 2.8 GB = 28.9 MB\n",
            "2022-10-06 18:45:56,292 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
            "2022-10-06 18:45:56,295 INFO namenode.FSDirectory: ACLs enabled? false\n",
            "2022-10-06 18:45:56,295 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
            "2022-10-06 18:45:56,295 INFO namenode.FSDirectory: XAttrs enabled? true\n",
            "2022-10-06 18:45:56,295 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
            "2022-10-06 18:45:56,299 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
            "2022-10-06 18:45:56,301 INFO snapshot.SnapshotManager: SkipList is disabled\n",
            "2022-10-06 18:45:56,305 INFO util.GSet: Computing capacity for map cachedBlocks\n",
            "2022-10-06 18:45:56,305 INFO util.GSet: VM type       = 64-bit\n",
            "2022-10-06 18:45:56,305 INFO util.GSet: 0.25% max memory 2.8 GB = 7.2 MB\n",
            "2022-10-06 18:45:56,305 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
            "2022-10-06 18:45:56,314 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
            "2022-10-06 18:45:56,314 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
            "2022-10-06 18:45:56,314 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
            "2022-10-06 18:45:56,318 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
            "2022-10-06 18:45:56,318 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
            "2022-10-06 18:45:56,320 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
            "2022-10-06 18:45:56,320 INFO util.GSet: VM type       = 64-bit\n",
            "2022-10-06 18:45:56,320 INFO util.GSet: 0.029999999329447746% max memory 2.8 GB = 887.0 KB\n",
            "2022-10-06 18:45:56,320 INFO util.GSet: capacity      = 2^17 = 131072 entries\n",
            "2022-10-06 18:45:56,353 INFO namenode.FSImage: Allocated new BlockPoolId: BP-80390672-172.28.0.2-1665081956338\n",
            "2022-10-06 18:45:56,378 INFO common.Storage: Storage directory /tmp/hadoop-root/dfs/name has been successfully formatted.\n",
            "2022-10-06 18:45:56,409 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n",
            "2022-10-06 18:45:56,512 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\n",
            "2022-10-06 18:45:56,529 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
            "2022-10-06 18:45:56,563 INFO namenode.FSNamesystem: Stopping services started for active state\n",
            "2022-10-06 18:45:56,564 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
            "2022-10-06 18:45:56,567 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
            "2022-10-06 18:45:56,567 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
            "/************************************************************\n",
            "SHUTDOWN_MSG: Shutting down NameNode at 7c99dff363e7/172.28.0.2\n",
            "************************************************************/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring Hadoop scripts available in sbin directory\n",
        "!ls $HADOOP_HOME/sbin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HOtQrP4UVMc",
        "outputId": "75714b48-985d-4766-a63d-3389369a3d5a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distribute-exclude.sh\t start-all.sh\t      stop-balancer.sh\n",
            "FederationStateStore\t start-balancer.sh    stop-dfs.cmd\n",
            "hadoop-daemon.sh\t start-dfs.cmd\t      stop-dfs.sh\n",
            "hadoop-daemons.sh\t start-dfs.sh\t      stop-secure-dns.sh\n",
            "httpfs.sh\t\t start-secure-dns.sh  stop-yarn.cmd\n",
            "kms.sh\t\t\t start-yarn.cmd       stop-yarn.sh\n",
            "mr-jobhistory-daemon.sh  start-yarn.sh\t      workers.sh\n",
            "refresh-namenodes.sh\t stop-all.cmd\t      yarn-daemon.sh\n",
            "start-all.cmd\t\t stop-all.sh\t      yarn-daemons.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating other necessary enviroment variables before starting nodes\n",
        "os.environ[\"HDFS_NAMENODE_USER\"] = \"root\"\n",
        "os.environ[\"HDFS_DATANODE_USER\"] = \"root\"\n",
        "os.environ[\"HDFS_SECONDARYNAMENODE_USER\"] = \"root\"\n",
        "os.environ[\"YARN_RESOURCEMANAGER_USER\"] = \"root\"\n",
        "os.environ[\"YARN_NODEMANAGER_USER\"] = \"root\""
      ],
      "metadata": {
        "id": "X9nIOwiJSIlX"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Launching hdfs deamon \n",
        "!$HADOOP_HOME/sbin/start-dfs.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ge9yaMGSXLA",
        "outputId": "d5237c71-179e-4c38-d85d-ff56168b41c9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting namenodes on [localhost]\n",
            "Starting datanodes\n",
            "Starting secondary namenodes [7c99dff363e7]\n",
            "7c99dff363e7: Warning: Permanently added '7c99dff363e7,172.28.0.2' (ECDSA) to the list of known hosts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing the running deamons\n",
        "!jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYJDpaOWSnFD",
        "outputId": "0d657bfa-d922-4309-b29b-1fd4eb857401"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1509 SecondaryNameNode\n",
            "1288 DataNode\n",
            "1161 NameNode\n",
            "1646 Jps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup $HADOOP_HOME/sbin/start-yarn.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ID_asMCXU6Cr",
        "outputId": "e79b6457-8385-4c20-f214-ffd7e2391949"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: ignoring input and appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/sbin/stop-yarn.sh\n",
        "!$HADOOP_HOME/sbin/stop-dfs.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFeI5tPwlAIC",
        "outputId": "1613b69d-e8d3-465b-d288-e767fa0a176b"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping nodemanagers\n",
            "Stopping resourcemanager\n",
            "Stopping namenodes on [localhost]\n",
            "Stopping datanodes\n",
            "Stopping secondary namenodes [3e881b068796]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing the running deamons\n",
        "!jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x09v8g6FU76T",
        "outputId": "927ac832-0dc9-4eea-947e-1752ba174b8f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2241 Jps\n",
            "1780 ResourceManager\n",
            "1509 SecondaryNameNode\n",
            "1288 DataNode\n",
            "1161 NameNode\n",
            "1915 NodeManager\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Report the basic file system information and statistics\n",
        "!$HADOOP_HOME/bin/hdfs dfsadmin -report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVxRN7ZnSqCq",
        "outputId": "044a3565-650f-4c1f-8547-cdbad797eb41"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configured Capacity: 115658190848 (107.72 GB)\n",
            "Present Capacity: 88488079360 (82.41 GB)\n",
            "DFS Remaining: 88488054784 (82.41 GB)\n",
            "DFS Used: 24576 (24 KB)\n",
            "DFS Used%: 0.00%\n",
            "Replicated Blocks:\n",
            "\tUnder replicated blocks: 0\n",
            "\tBlocks with corrupt replicas: 0\n",
            "\tMissing blocks: 0\n",
            "\tMissing blocks (with replication factor 1): 0\n",
            "\tLow redundancy blocks with highest priority to recover: 0\n",
            "\tPending deletion blocks: 0\n",
            "Erasure Coded Block Groups: \n",
            "\tLow redundancy block groups: 0\n",
            "\tBlock groups with corrupt internal blocks: 0\n",
            "\tMissing block groups: 0\n",
            "\tLow redundancy blocks with highest priority to recover: 0\n",
            "\tPending deletion blocks: 0\n",
            "\n",
            "-------------------------------------------------\n",
            "Live datanodes (1):\n",
            "\n",
            "Name: 127.0.0.1:9866 (localhost)\n",
            "Hostname: 7c99dff363e7\n",
            "Decommission Status : Normal\n",
            "Configured Capacity: 115658190848 (107.72 GB)\n",
            "DFS Used: 24576 (24 KB)\n",
            "Non DFS Used: 27153334272 (25.29 GB)\n",
            "DFS Remaining: 88488054784 (82.41 GB)\n",
            "DFS Used%: 0.00%\n",
            "DFS Remaining%: 76.51%\n",
            "Configured Cache Capacity: 0 (0 B)\n",
            "Cache Used: 0 (0 B)\n",
            "Cache Remaining: 0 (0 B)\n",
            "Cache Used%: 100.00%\n",
            "Cache Remaining%: 0.00%\n",
            "Xceivers: 1\n",
            "Last contact: Thu Oct 06 18:47:03 UTC 2022\n",
            "Last Block Report: Thu Oct 06 18:46:21 UTC 2022\n",
            "Num of Blocks: 0\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supervisión del clúster de Hadoop con la interfaz del navegador"
      ],
      "metadata": {
        "id": "aCvXXsqqS8ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "#The namenode posts the general report on port 9870\n",
        "output.serve_kernel_port_as_window(9870)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Jz-KO-w6S_Fd",
        "outputId": "9c0e421a-c016-460d-88ba-a6b1e1e613a6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(9870, \"/\", \"https://localhost:9870/\", window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejecutando el conteo de palabras en modo pseudodistribuido"
      ],
      "metadata": {
        "id": "RbVYpjSLTE4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget  https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCid70yETM04",
        "outputId": "545a702d-bf23-49de-9345-81ffda5b1fac"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-06 18:47:23--  https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt\n",
            "Resolving www.mirrorservice.org (www.mirrorservice.org)... 212.219.56.184, 2001:630:341:12::184\n",
            "Connecting to www.mirrorservice.org (www.mirrorservice.org)|212.219.56.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 678064 (662K) [text/plain]\n",
            "Saving to: ‘101.txt’\n",
            "\n",
            "101.txt             100%[===================>] 662.17K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2022-10-06 18:47:23 (8.91 MB/s) - ‘101.txt’ saved [678064/678064]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /word_count\n",
        "#Coping file from local file system to HDFS\n",
        "!$HADOOP_HOME/bin/hdfs dfs -put /content/101.txt /word_count"
      ],
      "metadata": {
        "id": "JVn15dxhTTRr"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wP3oYp54TUJp",
        "outputId": "d02cad42-6dc4-42a8-f15c-dcf62f3e3442"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 items\n",
            "-rw-r--r--   1 root supergroup     678064 2022-10-06 18:47 /word_count/101.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar wordcount /word_count/101.txt /word_count/output/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmXiVvQxTZT2",
        "outputId": "66483707-f1ef-49d7-c658-3a138b2e0008"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-10-06 18:48:21,631 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
            "2022-10-06 18:48:22,085 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1665082002873_0001\n",
            "2022-10-06 18:48:22,334 INFO input.FileInputFormat: Total input files to process : 1\n",
            "2022-10-06 18:48:22,441 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2022-10-06 18:48:23,010 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1665082002873_0001\n",
            "2022-10-06 18:48:23,011 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2022-10-06 18:48:23,188 INFO conf.Configuration: resource-types.xml not found\n",
            "2022-10-06 18:48:23,189 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
            "2022-10-06 18:48:23,488 INFO impl.YarnClientImpl: Submitted application application_1665082002873_0001\n",
            "2022-10-06 18:48:23,520 INFO mapreduce.Job: The url to track the job: http://7c99dff363e7:8088/proxy/application_1665082002873_0001/\n",
            "2022-10-06 18:48:23,520 INFO mapreduce.Job: Running job: job_1665082002873_0001\n",
            "2022-10-06 18:48:31,672 INFO mapreduce.Job: Job job_1665082002873_0001 running in uber mode : false\n",
            "2022-10-06 18:48:31,673 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2022-10-06 18:48:36,729 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2022-10-06 18:48:41,759 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2022-10-06 18:48:42,775 INFO mapreduce.Job: Job job_1665082002873_0001 completed successfully\n",
            "2022-10-06 18:48:42,864 INFO mapreduce.Job: Counters: 54\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=318005\n",
            "\t\tFILE: Number of bytes written=1108655\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=678169\n",
            "\t\tHDFS: Number of bytes written=233636\n",
            "\t\tHDFS: Number of read operations=8\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=2\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tJob Counters \n",
            "\t\tLaunched map tasks=1\n",
            "\t\tLaunched reduce tasks=1\n",
            "\t\tData-local map tasks=1\n",
            "\t\tTotal time spent by all maps in occupied slots (ms)=2948\n",
            "\t\tTotal time spent by all reduces in occupied slots (ms)=2617\n",
            "\t\tTotal time spent by all map tasks (ms)=2948\n",
            "\t\tTotal time spent by all reduce tasks (ms)=2617\n",
            "\t\tTotal vcore-milliseconds taken by all map tasks=2948\n",
            "\t\tTotal vcore-milliseconds taken by all reduce tasks=2617\n",
            "\t\tTotal megabyte-milliseconds taken by all map tasks=3018752\n",
            "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2679808\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=13006\n",
            "\t\tMap output records=105488\n",
            "\t\tMap output bytes=1078906\n",
            "\t\tMap output materialized bytes=318005\n",
            "\t\tInput split bytes=105\n",
            "\t\tCombine input records=105488\n",
            "\t\tCombine output records=21438\n",
            "\t\tReduce input groups=21438\n",
            "\t\tReduce shuffle bytes=318005\n",
            "\t\tReduce input records=21438\n",
            "\t\tReduce output records=21438\n",
            "\t\tSpilled Records=42876\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=115\n",
            "\t\tCPU time spent (ms)=2610\n",
            "\t\tPhysical memory (bytes) snapshot=515141632\n",
            "\t\tVirtual memory (bytes) snapshot=5187096576\n",
            "\t\tTotal committed heap usage (bytes)=532676608\n",
            "\t\tPeak Map Physical memory (bytes)=292511744\n",
            "\t\tPeak Map Virtual memory (bytes)=2594443264\n",
            "\t\tPeak Reduce Physical memory (bytes)=222629888\n",
            "\t\tPeak Reduce Virtual memory (bytes)=2592653312\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=678064\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=233636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count/output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rs2reQR5VHiE",
        "outputId": "2d8ca1c0-3bc4-421c-954d-4928626552d8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root supergroup          0 2022-10-06 18:48 /word_count/output/_SUCCESS\n",
            "-rw-r--r--   1 root supergroup     233636 2022-10-06 18:48 /word_count/output/part-r-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -cat /word_count/output/part-r-00000 | sort -rn -k2,2 |head -10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s6AnVTAVKIQ",
        "outputId": "d4018c5f-114e-4343-b9f7-61d8e1bf7267"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the\t5100\n",
            "of\t3356\n",
            "and\t2765\n",
            "a\t2434\n",
            "to\t2243\n",
            "in\t1693\n",
            "was\t1096\n",
            "is\t829\n",
            "for\t781\n",
            "that\t771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring Hadoop utilities available\n",
        "!ls $HADOOP_HOME/share/hadoop/tools/lib/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gg5voVtXVVfl",
        "outputId": "0131b243-bcea-42c1-cd98-5a8620bf1f4d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aliyun-java-sdk-core-4.5.10.jar      hadoop-gridmix-3.2.3.jar\n",
            "aliyun-java-sdk-kms-2.11.0.jar\t     hadoop-kafka-3.2.3.jar\n",
            "aliyun-java-sdk-ram-3.1.0.jar\t     hadoop-openstack-3.2.3.jar\n",
            "aliyun-sdk-oss-3.13.0.jar\t     hadoop-resourceestimator-3.2.3.jar\n",
            "aws-java-sdk-bundle-1.11.901.jar     hadoop-rumen-3.2.3.jar\n",
            "azure-data-lake-store-sdk-2.2.9.jar  hadoop-sls-3.2.3.jar\n",
            "azure-keyvault-core-1.0.0.jar\t     hadoop-streaming-3.2.3.jar\n",
            "azure-storage-7.0.0.jar\t\t     ini4j-0.5.4.jar\n",
            "hadoop-aliyun-3.2.3.jar\t\t     jdom2-2.0.6.jar\n",
            "hadoop-archive-logs-3.2.3.jar\t     kafka-clients-2.8.1.jar\n",
            "hadoop-archives-3.2.3.jar\t     lz4-java-1.7.1.jar\n",
            "hadoop-aws-3.2.3.jar\t\t     ojalgo-43.0.jar\n",
            "hadoop-azure-3.2.3.jar\t\t     opentracing-api-0.33.0.jar\n",
            "hadoop-azure-datalake-3.2.3.jar      opentracing-noop-0.33.0.jar\n",
            "hadoop-datajoin-3.2.3.jar\t     opentracing-util-0.33.0.jar\n",
            "hadoop-distcp-3.2.3.jar\t\t     org.jacoco.agent-0.8.5-runtime.jar\n",
            "hadoop-extras-3.2.3.jar\t\t     wildfly-openssl-1.0.7.Final.jar\n",
            "hadoop-fs2img-3.2.3.jar\t\t     zstd-jni-1.4.9-1.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /word_count_with_python\n",
        "!$HADOOP_HOME/bin/hdfs dfs -put /content/101.txt /word_count_with_python"
      ],
      "metadata": {
        "id": "6UwyQ7mPVZFj"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "\n",
        "# import sys because we need to read and write data to STDIN and STDOUT\n",
        "import sys\n",
        "  \n",
        "# reading entire line from STDIN (standard input)\n",
        "for line in sys.stdin:\n",
        "  line = line.strip()\n",
        "  words = line.split()\n",
        "  for word in words:\n",
        "    print('%s\\t%s' % (word, 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtQt8AM_VgnS",
        "outputId": "0b8a47b4-201b-44d2-8113-0b470de1d261"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "  \n",
        "from operator import itemgetter\n",
        "import sys\n",
        "  \n",
        "current_word = None\n",
        "current_count = 0\n",
        "word = None\n",
        "  \n",
        "# read the entire line from STDIN\n",
        "for line in sys.stdin:\n",
        "  # remove leading and trailing whitespace\n",
        "  line = line.strip()\n",
        "  # splitting the data on the basis of tab we have provided in mapper.py\n",
        "  word, count = line.split('\\t', 1)\n",
        "  # convert count (currently a string) to int\n",
        "  try:\n",
        "    count = int(count)\n",
        "  except ValueError:\n",
        "    # count was not a number, so silently\n",
        "    # ignore/discard this line\n",
        "    continue\n",
        "  \n",
        "  # this IF-switch only works because Hadoop sorts map output\n",
        "  # by key (here: word) before it is passed to the reducer\n",
        "  if current_word == word:\n",
        "    current_count += count\n",
        "  else:\n",
        "    if current_word: #to not print current_word=None\n",
        "      # write result to STDOUT\n",
        "      print('%s\\t%s' % (current_word, current_count))\n",
        "    current_count = count\n",
        "    current_word = word\n",
        "  \n",
        "# do not forget to output the last word if needed!\n",
        "if current_word == word:\n",
        "  print('%s\\t%s' % (current_word, current_count))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqBMkeeGVt-v",
        "outputId": "c67fb550-8f35-4d47-bca2-655969997485"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reducer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat 101.txt | python mapper.py | sort -k1,1 | python reducer.py | head -50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJnkx6B3VzDS",
        "outputId": "1e0c256f-c1af-41f3-df10-48569269d655"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~~~~~~~~~~~~~~\t1\n",
            "~~~~~~~~~~~~~~~~~~~~~\t2\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\t1\n",
            "=\t2\n",
            "_____________________________________\t1\n",
            "-\t39\n",
            "-----------\t1\n",
            "-------------\t1\n",
            "---------------\t1\n",
            "----------------------\t1\n",
            "----------------------------\t1\n",
            "---------------------------------\t1\n",
            "-----------------------------------\t1\n",
            ":\t1\n",
            ".\t271\n",
            ".'\t1\n",
            ".\"\t9\n",
            ".)\t8\n",
            "(.\t8\n",
            "@\t1\n",
            "**\t4\n",
            "***\t8\n",
            "*****\t2\n",
            "&\t10\n",
            "#\t31\n",
            "##\t2\n",
            "##!\t1\n",
            "+\t2\n",
            "+------------+\t2\n",
            "\"0\"\t1\n",
            "0.\t1\n",
            "00,\t1\n",
            "01,\t1\n",
            "\"02\"\t2\n",
            "02,\t1\n",
            "03,\t1\n",
            "04,\t1\n",
            "04,\"\t1\n",
            "05,\t1\n",
            "0-553-08058-X,\t1\n",
            "0-553-56370-X.\t1\n",
            "06-11-91\t53\n",
            "08-03-90\t1\n",
            "1\t4\n",
            "\"1.\t1\n",
            "($1\t1\n",
            "1,\t5\n",
            "1:\t1\n",
            "1.\t6\n",
            "1)\t1\n",
            "Traceback (most recent call last):\n",
            "  File \"reducer.py\", line 32, in <module>\n",
            "    print('%s\\t%s' % (current_word, current_count))\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 777 /content/mapper.py /content/reducer.py"
      ],
      "metadata": {
        "id": "xe3NHB1nWBGw"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running MapReduce programs\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar \\\n",
        "  -input /word_count_with_python/101.txt \\\n",
        "  -output /word_count_with_python/output \\\n",
        "  -mapper \"python /content/mapper.py\" \\\n",
        "  -reducer \"python /content/reducer.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr6JX-viWFDo",
        "outputId": "0611bf7d-e175-4604-d9c8-9bf2c36d0846"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "packageJobJar: [/tmp/hadoop-unjar8618387521944422567/] [] /tmp/streamjob40920479028461194.jar tmpDir=null\n",
            "2022-10-06 18:54:42,815 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
            "2022-10-06 18:54:43,052 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
            "2022-10-06 18:54:43,345 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1665082002873_0002\n",
            "2022-10-06 18:54:44,003 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2022-10-06 18:54:44,102 INFO mapreduce.JobSubmitter: number of splits:2\n",
            "2022-10-06 18:54:44,296 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1665082002873_0002\n",
            "2022-10-06 18:54:44,298 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2022-10-06 18:54:44,484 INFO conf.Configuration: resource-types.xml not found\n",
            "2022-10-06 18:54:44,484 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
            "2022-10-06 18:54:44,587 INFO impl.YarnClientImpl: Submitted application application_1665082002873_0002\n",
            "2022-10-06 18:54:44,661 INFO mapreduce.Job: The url to track the job: http://7c99dff363e7:8088/proxy/application_1665082002873_0002/\n",
            "2022-10-06 18:54:44,664 INFO mapreduce.Job: Running job: job_1665082002873_0002\n",
            "2022-10-06 18:54:51,781 INFO mapreduce.Job: Job job_1665082002873_0002 running in uber mode : false\n",
            "2022-10-06 18:54:51,782 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2022-10-06 18:54:59,884 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2022-10-06 18:55:05,929 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2022-10-06 18:55:05,939 INFO mapreduce.Job: Job job_1665082002873_0002 completed successfully\n",
            "2022-10-06 18:55:06,031 INFO mapreduce.Job: Counters: 54\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1078912\n",
            "\t\tFILE: Number of bytes written=2873173\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=682368\n",
            "\t\tHDFS: Number of bytes written=233636\n",
            "\t\tHDFS: Number of read operations=11\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=2\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tJob Counters \n",
            "\t\tLaunched map tasks=2\n",
            "\t\tLaunched reduce tasks=1\n",
            "\t\tData-local map tasks=2\n",
            "\t\tTotal time spent by all maps in occupied slots (ms)=11898\n",
            "\t\tTotal time spent by all reduces in occupied slots (ms)=2903\n",
            "\t\tTotal time spent by all map tasks (ms)=11898\n",
            "\t\tTotal time spent by all reduce tasks (ms)=2903\n",
            "\t\tTotal vcore-milliseconds taken by all map tasks=11898\n",
            "\t\tTotal vcore-milliseconds taken by all reduce tasks=2903\n",
            "\t\tTotal megabyte-milliseconds taken by all map tasks=12183552\n",
            "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2972672\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=13006\n",
            "\t\tMap output records=105488\n",
            "\t\tMap output bytes=867930\n",
            "\t\tMap output materialized bytes=1078918\n",
            "\t\tInput split bytes=208\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=21438\n",
            "\t\tReduce shuffle bytes=1078918\n",
            "\t\tReduce input records=105488\n",
            "\t\tReduce output records=21438\n",
            "\t\tSpilled Records=210976\n",
            "\t\tShuffled Maps =2\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=2\n",
            "\t\tGC time elapsed (ms)=337\n",
            "\t\tCPU time spent (ms)=3930\n",
            "\t\tPhysical memory (bytes) snapshot=818155520\n",
            "\t\tVirtual memory (bytes) snapshot=7760678912\n",
            "\t\tTotal committed heap usage (bytes)=779091968\n",
            "\t\tPeak Map Physical memory (bytes)=321089536\n",
            "\t\tPeak Map Virtual memory (bytes)=2586632192\n",
            "\t\tPeak Reduce Physical memory (bytes)=177152000\n",
            "\t\tPeak Reduce Virtual memory (bytes)=2589876224\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=682160\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=233636\n",
            "2022-10-06 18:55:06,031 INFO streaming.StreamJob: Output directory: /word_count_with_python/output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -cat /word_count_with_python/output/part-00000 | sort -rn -k2,2 | head -10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgQm9w9uWNc4",
        "outputId": "cb26e45b-956d-4feb-d114-539aae658839"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the\t5100\n",
            "of\t3356\n",
            "and\t2765\n",
            "a\t2434\n",
            "to\t2243\n",
            "in\t1693\n",
            "was\t1096\n",
            "is\t829\n",
            "for\t781\n",
            "that\t771\n"
          ]
        }
      ]
    }
  ]
}