{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0qfUgnDpnymNb4AAqGuEB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adigenova/uohpmd/blob/main/code/HadoopII.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Java 8\n",
        "Hadoop es un framework de procesamiento de datos basado en java."
      ],
      "metadata": {
        "id": "khCyTGqeEj-0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntUhLC8XEcCN",
        "outputId": "d0a2fbe5-7baf-41a8-b59e-2d99d2d9ef0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.20.1\" 2023-08-24\n",
            "OpenJDK Runtime Environment (build 11.0.20.1+1-post-Ubuntu-0ubuntu122.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.20.1+1-post-Ubuntu-0ubuntu122.04, mixed mode, sharing)\n"
          ]
        }
      ],
      "source": [
        "!java -version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5PpGZooE786",
        "outputId": "61edda71-49d1-467c-a7dd-dfc48719a796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum fonts-ipafont-gothic\n",
            "  fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 18 not upgraded.\n",
            "Need to get 39.7 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-8-jre-headless amd64 8u382-ga-1~22.04.1 [30.8 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-8-jdk-headless amd64 8u382-ga-1~22.04.1 [8,851 kB]\n",
            "Fetched 39.7 MB in 3s (12.6 MB/s)\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 120875 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u382-ga-1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u382-ga-1~22.04.1) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u382-ga-1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u382-ga-1~22.04.1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u382-ga-1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u382-ga-1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cambiamos la version de java pro defecto (elegir 2)\n",
        "!update-alternatives --config java"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6M4pBv9FPRW",
        "outputId": "91a15f04-d58d-442b-c502-0be246a1e453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative java (providing /usr/bin/java).\n",
            "\n",
            "  Selection    Path                                            Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java   1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!update-alternatives --config javac"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVqlFyjCFdDZ",
        "outputId": "957dd974-286f-4506-c119-fef2b139e751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative javac (providing /usr/bin/javac).\n",
            "\n",
            "  Selection    Path                                          Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/javac   1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/javac   1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/bin/javac    1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!update-alternatives --config jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTzRtnRcFjkl",
        "outputId": "8a190a56-e5a5-41a6-d2ae-d1d06fe41a86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative jps (providing /usr/bin/jps).\n",
            "\n",
            "  Selection    Path                                        Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/jps   1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/jps   1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/bin/jps    1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3NMuLCuFqaF",
        "outputId": "85a2d0bb-69a7-4f72-ae01-3d33fcae54f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"1.8.0_382\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_382-8u382-ga-1~22.04.1-b05)\n",
            "OpenJDK 64-Bit Server VM (build 25.382-b05, mixed mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creación de variables de entorno relacionadas con Java\n",
        "\n",
        "JAVA_HOME es una variable de entorno del sistema operativo que apunta a la ubicación del sistema de archivos donde se instaló JDK o JRE."
      ],
      "metadata": {
        "id": "mnELy7FzF32P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#en contrando el path de java\n",
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRjK3MR7FxQG",
        "outputId": "999e4d81-e058-43a3-ded0-868673815692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-8-openjdk-amd64/jre/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importando el module os\n",
        "import os\n",
        "#creando las variables de ambiente\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"JRE_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/jre\"\n",
        "os.environ[\"PATH\"] += \":$JAVA_HOME/bin:$JRE_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\""
      ],
      "metadata": {
        "id": "7xOy0zg7F9hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalando SSH\n",
        "\n"
      ],
      "metadata": {
        "id": "pXhaEYZ4Kwun"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necesitamos definir un medio para que el nodo maestro acceda de forma remota a todos los nodos de nuestro clúster.\n",
        "\n",
        "Hadoop utiliza frases de contraseña SSH para la comunicación entre los nodos.\n",
        "\n",
        "SSH es un protocolo de red criptográfico para operar servicios de red de forma segura en una red no segura.\n",
        "\n",
        "SSH utiliza criptografía de clave pública estándar para crear un par de claves para la verificación del usuario: una pública y otra privada"
      ],
      "metadata": {
        "id": "PRke9v59K8hV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openssh-server"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l60zLteZKzFM",
        "outputId": "89a00b48-0d03-4b5c-c125-c299637b770a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libwrap0 ncurses-term openssh-sftp-server ssh-import-id\n",
            "Suggested packages:\n",
            "  molly-guard monkeysphere ssh-askpass ufw\n",
            "The following NEW packages will be installed:\n",
            "  libwrap0 ncurses-term openssh-server openssh-sftp-server ssh-import-id\n",
            "0 upgraded, 5 newly installed, 0 to remove and 18 not upgraded.\n",
            "Need to get 799 kB of archives.\n",
            "After this operation, 6,157 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openssh-sftp-server amd64 1:8.9p1-3ubuntu0.4 [38.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwrap0 amd64 7.6.q-31build2 [47.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openssh-server amd64 1:8.9p1-3ubuntu0.4 [434 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ncurses-term all 6.3-2ubuntu0.1 [267 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 ssh-import-id all 5.11-0ubuntu1 [10.1 kB]\n",
            "Fetched 799 kB in 1s (1,152 kB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package openssh-sftp-server.\n",
            "(Reading database ... 121225 files and directories currently installed.)\n",
            "Preparing to unpack .../openssh-sftp-server_1%3a8.9p1-3ubuntu0.4_amd64.deb ...\n",
            "Unpacking openssh-sftp-server (1:8.9p1-3ubuntu0.4) ...\n",
            "Selecting previously unselected package libwrap0:amd64.\n",
            "Preparing to unpack .../libwrap0_7.6.q-31build2_amd64.deb ...\n",
            "Unpacking libwrap0:amd64 (7.6.q-31build2) ...\n",
            "Selecting previously unselected package openssh-server.\n",
            "Preparing to unpack .../openssh-server_1%3a8.9p1-3ubuntu0.4_amd64.deb ...\n",
            "Unpacking openssh-server (1:8.9p1-3ubuntu0.4) ...\n",
            "Selecting previously unselected package ncurses-term.\n",
            "Preparing to unpack .../ncurses-term_6.3-2ubuntu0.1_all.deb ...\n",
            "Unpacking ncurses-term (6.3-2ubuntu0.1) ...\n",
            "Selecting previously unselected package ssh-import-id.\n",
            "Preparing to unpack .../ssh-import-id_5.11-0ubuntu1_all.deb ...\n",
            "Unpacking ssh-import-id (5.11-0ubuntu1) ...\n",
            "Setting up openssh-sftp-server (1:8.9p1-3ubuntu0.4) ...\n",
            "Setting up ssh-import-id (5.11-0ubuntu1) ...\n",
            "Setting up libwrap0:amd64 (7.6.q-31build2) ...\n",
            "Setting up ncurses-term (6.3-2ubuntu0.1) ...\n",
            "Setting up openssh-server (1:8.9p1-3ubuntu0.4) ...\n",
            "\n",
            "Creating config file /etc/ssh/sshd_config with new version\n",
            "Creating SSH2 RSA key; this may take some time ...\n",
            "3072 SHA256:dWA3ruGBhnQkxO4x8TtM/6/IG6B2oKOhOj2jhw7Z/Ow root@b12352b94bc7 (RSA)\n",
            "Creating SSH2 ECDSA key; this may take some time ...\n",
            "256 SHA256:QalspbE27szXxMbKi0NUCv9jiDUA4CYpFAbbkHwvOh4 root@b12352b94bc7 (ECDSA)\n",
            "Creating SSH2 ED25519 key; this may take some time ...\n",
            "256 SHA256:cdQBtsIbdf9giCouJak+VU6zMjTg3IqMbRKUOifQ3tM root@b12352b94bc7 (ED25519)\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Created symlink /etc/systemd/system/sshd.service → /lib/systemd/system/ssh.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/ssh.service → /lib/systemd/system/ssh.service.\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Starting the server\n",
        "!service ssh start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7EDXyoSLAgi",
        "outputId": "c2b7829e-c0cb-474f-9d40-614ffd1f5aaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Starting OpenBSD Secure Shell server sshd\n",
            "   ...done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep Port /etc/ssh/sshd_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNKXUeCzLFqO",
        "outputId": "d097738a-e1d1-4095-b9a7-af3c7ed382b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Port 22\n",
            "#GatewayPorts no\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzq6op9-LQRq",
        "outputId": "466a48d0-5cda-422b-fec4-71147b8afeb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating public/private rsa key pair.\n",
            "Created directory '/root/.ssh'.\n",
            "Your identification has been saved in /root/.ssh/id_rsa\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub\n",
            "The key fingerprint is:\n",
            "SHA256:QYNVJWNhG3nYhJfiyvmPNs6vgcztBx8q8LuHz90cm9g root@b12352b94bc7\n",
            "The key's randomart image is:\n",
            "+---[RSA 3072]----+\n",
            "|       o+.OOo.   |\n",
            "|      .. +===    |\n",
            "|        ...+     |\n",
            "|         ..      |\n",
            "|       .So       |\n",
            "|      .o+o. .    |\n",
            "|       o+oo+ ..  |\n",
            "|        ++*+o= + |\n",
            "|        oOO*= E  |\n",
            "+----[SHA256]-----+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Copying the key to autorized keys\n",
        "!cat $HOME/.ssh/id_rsa.pub>>$HOME/.ssh/authorized_keys\n",
        "#Changing the permissions on the key\n",
        "!chmod 0600 ~/.ssh/authorized_keys"
      ],
      "metadata": {
        "id": "N0e-UAsPLUZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Conneting with the local machine\n",
        "!ssh -o StrictHostKeyChecking=no localhost uptime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhI5w6TGLXpR",
        "outputId": "6c6f4993-5209-4041-8a1e-085e08e62830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\r\n",
            " 14:34:51 up 3 min,  0 users,  load average: 0.45, 0.31, 0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Hadoop 3.2.3"
      ],
      "metadata": {
        "id": "Xo_paSdCLc7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget -c  https://archive.apache.org/dist/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz\n",
        "#!wget https://archive.apache.org/dist/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz\n",
        "!wget https://dlcdn.apache.org/hadoop/common/hadoop-3.2.4/hadoop-3.2.4.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB_eAifNLeuH",
        "outputId": "ce97f525-1128-456b-e343-12931b64c52c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-16 14:40:42--  https://dlcdn.apache.org/hadoop/common/hadoop-3.2.4/hadoop-3.2.4.tar.gz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 492368219 (470M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.2.4.tar.gz’\n",
            "\n",
            "hadoop-3.2.4.tar.gz 100%[===================>] 469.56M   256MB/s    in 1.8s    \n",
            "\n",
            "2023-10-16 14:41:04 (256 MB/s) - ‘hadoop-3.2.4.tar.gz’ saved [492368219/492368219]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo tar -xzf hadoop-3.2.4.tar.gz\n",
        "!cp -r hadoop-3.2.4/ /usr/local/\n",
        "!ls /usr/local/hadoop-3.2.4/etc/hadoop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1BMqUQtMNxi",
        "outputId": "a56fe094-9ed5-4723-9822-d9f28fff109e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "capacity-scheduler.xml\t\t  httpfs-log4j.properties     mapred-site.xml\n",
            "configuration.xsl\t\t  httpfs-signature.secret     shellprofile.d\n",
            "container-executor.cfg\t\t  httpfs-site.xml\t      ssl-client.xml.example\n",
            "core-site.xml\t\t\t  kms-acls.xml\t\t      ssl-server.xml.example\n",
            "hadoop-env.cmd\t\t\t  kms-env.sh\t\t      user_ec_policies.xml.template\n",
            "hadoop-env.sh\t\t\t  kms-log4j.properties\t      workers\n",
            "hadoop-metrics2.properties\t  kms-site.xml\t\t      yarn-env.cmd\n",
            "hadoop-policy.xml\t\t  log4j.properties\t      yarn-env.sh\n",
            "hadoop-user-functions.sh.example  mapred-env.cmd\t      yarnservice-log4j.properties\n",
            "hdfs-site.xml\t\t\t  mapred-env.sh\t\t      yarn-site.xml\n",
            "httpfs-env.sh\t\t\t  mapred-queues.xml.template\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i '/export JAVA_HOME=/a export JAVA_HOME=\\/usr\\/lib\\/jvm\\/java-8-openjdk-amd64' /usr/local/hadoop-3.2.4/etc/hadoop/hadoop-env.sh"
      ],
      "metadata": {
        "id": "JTMXlQv_Mhc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.2.4\""
      ],
      "metadata": {
        "id": "7FE7MO-ZMpds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls $HADOOP_HOME/etc/hadoop/*.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWlny-uxMvPc",
        "outputId": "11a253a2-d35d-48a3-c56b-c18b1abb7a1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/hadoop-3.2.4/etc/hadoop/capacity-scheduler.xml\n",
            "/usr/local/hadoop-3.2.4/etc/hadoop/core-site.xml\n",
            "/usr/local/hadoop-3.2.4/etc/hadoop/hadoop-policy.xml\n",
            "/usr/local/hadoop-3.2.4/etc/hadoop/hdfs-site.xml\n",
            "/usr/local/hadoop-3.2.4/etc/hadoop/httpfs-site.xml\n",
            "/usr/local/hadoop-3.2.4/etc/hadoop/kms-acls.xml\n",
            "/usr/local/hadoop-3.2.4/etc/hadoop/kms-site.xml\n",
            "/usr/local/hadoop-3.2.4/etc/hadoop/mapred-site.xml\n",
            "/usr/local/hadoop-3.2.4/etc/hadoop/yarn-site.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat $HADOOP_HOME/etc/hadoop/core-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW_AfNd6M9X4",
        "outputId": "1f8be5f0-4ca0-472d-b02f-42cc22eeff77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "\n",
            "<!-- Put site-specific property overrides in this file. -->\n",
            "\n",
            "<configuration>\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Hadoop in standalone mode\n",
        "\n",
        "Con las propiedades de configuración predeterminadas, Hadoop se ejecuta en modo independiente (modo no distribuido). Es decir, el modo independiente (también conocido como modo local) es el modo predeterminado para Hadoop.\n",
        "\n",
        "No hay demonios para ejecutar. Solo un solo proceso java\n",
        "\n",
        "Se utilizan el sistema de archivos local y el ejecutor de trabajos MapReduce local\n",
        "\n",
        "El comando para ejecutar un programa Hadoop mapreduce que está escrito en Java es:\n",
        "\n",
        "```\n",
        "$HADOOP_HOME/bin/hadoop jar <jar>\n",
        "```"
      ],
      "metadata": {
        "id": "QNb_VOY3M-4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls $HADOOP_HOME/share/hadoop/mapreduce/*.jar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZf6kqikNUB2",
        "outputId": "55a2c522-bee7-450b-9d83-f79f26628bcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/hadoop-3.2.4/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.4.jar\n",
            "/usr/local/hadoop-3.2.4/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.4.jar\n",
            "/usr/local/hadoop-3.2.4/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.4.jar\n",
            "/usr/local/hadoop-3.2.4/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.4.jar\n",
            "/usr/local/hadoop-3.2.4/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.4.jar\n",
            "/usr/local/hadoop-3.2.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.4.jar\n",
            "/usr/local/hadoop-3.2.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.4-tests.jar\n",
            "/usr/local/hadoop-3.2.4/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.4.jar\n",
            "/usr/local/hadoop-3.2.4/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.4.jar\n",
            "/usr/local/hadoop-3.2.4/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.4.jar\n",
            "/usr/local/hadoop-3.2.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.4.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplos Map/reduce"
      ],
      "metadata": {
        "id": "wzpaLSzFNj3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.4.jar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VePKmgiINmPz",
        "outputId": "a4e7e0d6-c998-44e4-e711-f3203371f416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An example program must be given as the first argument.\n",
            "Valid program names are:\n",
            "  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.\n",
            "  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.\n",
            "  bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.\n",
            "  dbcount: An example job that count the pageview counts from a database.\n",
            "  distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.\n",
            "  grep: A map/reduce program that counts the matches of a regex in the input.\n",
            "  join: A job that effects a join over sorted, equally partitioned datasets\n",
            "  multifilewc: A job that counts words from several files.\n",
            "  pentomino: A map/reduce tile laying program to find solutions to pentomino problems.\n",
            "  pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.\n",
            "  randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.\n",
            "  randomwriter: A map/reduce program that writes 10GB of random data per node.\n",
            "  secondarysort: An example defining a secondary sort to the reduce.\n",
            "  sort: A map/reduce program that sorts the data written by the random writer.\n",
            "  sudoku: A sudoku solver.\n",
            "  teragen: Generate data for the terasort\n",
            "  terasort: Run the terasort\n",
            "  teravalidate: Checking results of terasort\n",
            "  wordcount: A map/reduce program that counts the words in the input files.\n",
            "  wordmean: A map/reduce program that counts the average length of the words in the input files.\n",
            "  wordmedian: A map/reduce program that counts the median length of the words in the input files.\n",
            "  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.4.jar wordcount"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQGDQcsuNvu2",
        "outputId": "aef83408-3fdc-487a-af12-d12948279dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: wordcount <in> [<in>...] <out>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget  https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Cl1ICh5N3Vk",
        "outputId": "6c955db9-5c8b-4a71-c916-20865012784d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-16 14:42:58--  https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt\n",
            "Resolving www.mirrorservice.org (www.mirrorservice.org)... 212.219.56.184, 2001:630:341:12::184\n",
            "Connecting to www.mirrorservice.org (www.mirrorservice.org)|212.219.56.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 678064 (662K) [text/plain]\n",
            "Saving to: ‘101.txt’\n",
            "\n",
            "101.txt             100%[===================>] 662.17K   935KB/s    in 0.7s    \n",
            "\n",
            "2023-10-16 14:43:00 (935 KB/s) - ‘101.txt’ saved [678064/678064]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.4.jar \\\n",
        "wordcount /content/101.txt /content/output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPqP2imoOBAk",
        "outputId": "fe263f8a-19d9-49c4-a881-eddcf2e802c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-16 14:43:11,950 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2023-10-16 14:43:12,190 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2023-10-16 14:43:12,190 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2023-10-16 14:43:12,390 INFO input.FileInputFormat: Total input files to process : 1\n",
            "2023-10-16 14:43:12,434 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2023-10-16 14:43:12,709 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local80144435_0001\n",
            "2023-10-16 14:43:12,710 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2023-10-16 14:43:12,918 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2023-10-16 14:43:12,919 INFO mapreduce.Job: Running job: job_local80144435_0001\n",
            "2023-10-16 14:43:12,926 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2023-10-16 14:43:12,934 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-10-16 14:43:12,934 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-10-16 14:43:12,935 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "2023-10-16 14:43:12,978 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2023-10-16 14:43:12,979 INFO mapred.LocalJobRunner: Starting task: attempt_local80144435_0001_m_000000_0\n",
            "2023-10-16 14:43:13,008 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-10-16 14:43:13,008 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-10-16 14:43:13,035 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-10-16 14:43:13,045 INFO mapred.MapTask: Processing split: file:/content/101.txt:0+678064\n",
            "2023-10-16 14:43:13,212 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2023-10-16 14:43:13,212 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2023-10-16 14:43:13,212 INFO mapred.MapTask: soft limit at 83886080\n",
            "2023-10-16 14:43:13,212 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2023-10-16 14:43:13,212 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2023-10-16 14:43:13,218 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2023-10-16 14:43:13,522 INFO mapred.LocalJobRunner: \n",
            "2023-10-16 14:43:13,523 INFO mapred.MapTask: Starting flush of map output\n",
            "2023-10-16 14:43:13,523 INFO mapred.MapTask: Spilling map output\n",
            "2023-10-16 14:43:13,523 INFO mapred.MapTask: bufstart = 0; bufend = 1078906; bufvoid = 104857600\n",
            "2023-10-16 14:43:13,523 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25792448(103169792); length = 421949/6553600\n",
            "2023-10-16 14:43:13,924 INFO mapreduce.Job: Job job_local80144435_0001 running in uber mode : false\n",
            "2023-10-16 14:43:13,925 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2023-10-16 14:43:14,126 INFO mapred.MapTask: Finished spill 0\n",
            "2023-10-16 14:43:14,143 INFO mapred.Task: Task:attempt_local80144435_0001_m_000000_0 is done. And is in the process of committing\n",
            "2023-10-16 14:43:14,147 INFO mapred.LocalJobRunner: map\n",
            "2023-10-16 14:43:14,147 INFO mapred.Task: Task 'attempt_local80144435_0001_m_000000_0' done.\n",
            "2023-10-16 14:43:14,158 INFO mapred.Task: Final Counters for attempt_local80144435_0001_m_000000_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=994677\n",
            "\t\tFILE: Number of bytes written=1179415\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=13006\n",
            "\t\tMap output records=105488\n",
            "\t\tMap output bytes=1078906\n",
            "\t\tMap output materialized bytes=318005\n",
            "\t\tInput split bytes=86\n",
            "\t\tCombine input records=105488\n",
            "\t\tCombine output records=21438\n",
            "\t\tSpilled Records=21438\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=258473984\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=678064\n",
            "2023-10-16 14:43:14,159 INFO mapred.LocalJobRunner: Finishing task: attempt_local80144435_0001_m_000000_0\n",
            "2023-10-16 14:43:14,160 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2023-10-16 14:43:14,171 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2023-10-16 14:43:14,171 INFO mapred.LocalJobRunner: Starting task: attempt_local80144435_0001_r_000000_0\n",
            "2023-10-16 14:43:14,184 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-10-16 14:43:14,184 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-10-16 14:43:14,185 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-10-16 14:43:14,190 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3a547f52\n",
            "2023-10-16 14:43:14,193 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2023-10-16 14:43:14,224 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2117966208, maxSingleShuffleLimit=529491552, mergeThreshold=1397857792, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2023-10-16 14:43:14,242 INFO reduce.EventFetcher: attempt_local80144435_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2023-10-16 14:43:14,283 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local80144435_0001_m_000000_0 decomp: 318001 len: 318005 to MEMORY\n",
            "2023-10-16 14:43:14,288 INFO reduce.InMemoryMapOutput: Read 318001 bytes from map-output for attempt_local80144435_0001_m_000000_0\n",
            "2023-10-16 14:43:14,289 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 318001, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->318001\n",
            "2023-10-16 14:43:14,292 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2023-10-16 14:43:14,293 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2023-10-16 14:43:14,293 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2023-10-16 14:43:14,303 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2023-10-16 14:43:14,304 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 317994 bytes\n",
            "2023-10-16 14:43:14,342 INFO reduce.MergeManagerImpl: Merged 1 segments, 318001 bytes to disk to satisfy reduce memory limit\n",
            "2023-10-16 14:43:14,343 INFO reduce.MergeManagerImpl: Merging 1 files, 318005 bytes from disk\n",
            "2023-10-16 14:43:14,344 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2023-10-16 14:43:14,344 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2023-10-16 14:43:14,345 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 317994 bytes\n",
            "2023-10-16 14:43:14,346 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2023-10-16 14:43:14,349 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2023-10-16 14:43:14,514 INFO mapred.Task: Task:attempt_local80144435_0001_r_000000_0 is done. And is in the process of committing\n",
            "2023-10-16 14:43:14,519 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2023-10-16 14:43:14,520 INFO mapred.Task: Task attempt_local80144435_0001_r_000000_0 is allowed to commit now\n",
            "2023-10-16 14:43:14,523 INFO output.FileOutputCommitter: Saved output of task 'attempt_local80144435_0001_r_000000_0' to file:/content/output\n",
            "2023-10-16 14:43:14,525 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2023-10-16 14:43:14,525 INFO mapred.Task: Task 'attempt_local80144435_0001_r_000000_0' done.\n",
            "2023-10-16 14:43:14,528 INFO mapred.Task: Final Counters for attempt_local80144435_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1630719\n",
            "\t\tFILE: Number of bytes written=1732892\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=21438\n",
            "\t\tReduce shuffle bytes=318005\n",
            "\t\tReduce input records=21438\n",
            "\t\tReduce output records=21438\n",
            "\t\tSpilled Records=21438\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=30\n",
            "\t\tTotal committed heap usage (bytes)=258473984\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=235472\n",
            "2023-10-16 14:43:14,528 INFO mapred.LocalJobRunner: Finishing task: attempt_local80144435_0001_r_000000_0\n",
            "2023-10-16 14:43:14,528 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2023-10-16 14:43:14,928 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2023-10-16 14:43:14,929 INFO mapreduce.Job: Job job_local80144435_0001 completed successfully\n",
            "2023-10-16 14:43:14,941 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=2625396\n",
            "\t\tFILE: Number of bytes written=2912307\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=13006\n",
            "\t\tMap output records=105488\n",
            "\t\tMap output bytes=1078906\n",
            "\t\tMap output materialized bytes=318005\n",
            "\t\tInput split bytes=86\n",
            "\t\tCombine input records=105488\n",
            "\t\tCombine output records=21438\n",
            "\t\tReduce input groups=21438\n",
            "\t\tReduce shuffle bytes=318005\n",
            "\t\tReduce input records=21438\n",
            "\t\tReduce output records=21438\n",
            "\t\tSpilled Records=42876\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=30\n",
            "\t\tTotal committed heap usage (bytes)=516947968\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=678064\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=235472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DZD89ZZORB1",
        "outputId": "60fabfa8-0822-462f-b837-47007fdb1d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-r-00000  _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -50 /content/output/part-r-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMG4IkXROXIs",
        "outputId": "76590e2f-4362-40bf-a4ef-200b8ab9da85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"(d)\t1\n",
            "\"/H\"\t1\n",
            "\"0\"\t1\n",
            "\"02\"\t2\n",
            "\"1.\t1\n",
            "\"414\t3\n",
            "\"A\t2\n",
            "\"AT&T\t3\n",
            "\"AT&T's\t1\n",
            "\"Access\t1\n",
            "\"Acid\t3\n",
            "\"Ad-hocracy\"\t1\n",
            "\"Advanced\t1\n",
            "\"Agents\t1\n",
            "\"Al\t3\n",
            "\"All\t3\n",
            "\"American\t1\n",
            "\"An\t1\n",
            "\"And\t2\n",
            "\"Any\t1\n",
            "\"Are\t3\n",
            "\"Artificial\t1\n",
            "\"As\t1\n",
            "\"Assistant\t1\n",
            "\"Attctc\"\t2\n",
            "\"Auld\t1\n",
            "\"Autodesk,\"\t1\n",
            "\"BBS,\"\t2\n",
            "\"BIRTHPLACE\t1\n",
            "\"BRITS\t1\n",
            "\"Barry\t1\n",
            "\"Because\t1\n",
            "\"Before\t1\n",
            "\"Bell\t2\n",
            "\"Bell\"\t1\n",
            "\"BellSouth\t2\n",
            "\"Berkeley\t1\n",
            "\"Big\t1\n",
            "\"Biggest\t1\n",
            "\"Black\t3\n",
            "\"Blue\t1\n",
            "\"Bob\"\t1\n",
            "\"Bob,\t3\n",
            "\"Bob:\t1\n",
            "\"Bullet-N-Board.\"\t1\n",
            "\"Bureaucrat-ese\t1\n",
            "\"But\t4\n",
            "\"C-word.\"\t1\n",
            "\"CALIFORNIA\"\t1\n",
            "\"CC,\"\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cat /content/output/part-r-00000 | sort -rn -k2,2 | head -n 20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7ONdp1LOckJ",
        "outputId": "7b420d4c-b43f-448b-caf0-2aed05d70794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the\t5100\n",
            "of\t3356\n",
            "and\t2765\n",
            "a\t2434\n",
            "to\t2243\n",
            "in\t1693\n",
            "was\t1096\n",
            "is\t829\n",
            "for\t781\n",
            "that\t771\n",
            "The\t751\n",
            "had\t719\n",
            "with\t641\n",
            "on\t594\n",
            "as\t572\n",
            "by\t539\n",
            "it\t504\n",
            "this\t489\n",
            "not\t486\n",
            "are\t477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Hadoop in Pseudo-distributed mode\n",
        "\n",
        "En el modo Pseudodistribuido entran en juego todos los componentes distribuidos de Hadoop. Es decir, todos los demonios de Hadoop responsables del almacenamiento distribuido y el procesamiento distribuido se ejecutarán en la misma máquina.\n",
        "\n",
        "Demonios maestros:\n",
        "\n",
        "1. NameNode\n",
        "2. Resource Manager\n",
        "3. Standby NameNode\n",
        "\n",
        "Demonios esclavos:\n",
        "\n",
        "1. DataNode\n",
        "2. Node Manager"
      ],
      "metadata": {
        "id": "JCwjdIOwPIqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding required property to core-site.xlm file\n",
        "!sed -i '/<configuration>/a\\\n",
        "  <property>\\n\\\n",
        "    <name>fs.defaultFS</name>\\n\\\n",
        "    <value>hdfs://localhost:9000</value>\\n\\\n",
        "  </property>' \\\n",
        "$HADOOP_HOME/etc/hadoop/core-site.xml\n"
      ],
      "metadata": {
        "id": "q0tS2ZyyRqnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat $HADOOP_HOME/etc/hadoop/core-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnp90asyR3hi",
        "outputId": "febd2f70-b099-4f99-e9e7-ff84041fd26e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "\n",
            "<!-- Put site-specific property overrides in this file. -->\n",
            "\n",
            "<configuration>\n",
            "<property>\n",
            "     <name>fs.defaultFS</name>\n",
            "     <value>hdfs://localhost:9000</value>\n",
            "   </property>\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i '/<configuration>/a\\\n",
        "  <property>\\n\\\n",
        "    <name>dfs.replication</name>\\n\\\n",
        "    <value>1</value>\\n\\\n",
        "  </property>' \\\n",
        "$HADOOP_HOME/etc/hadoop/hdfs-site.xml"
      ],
      "metadata": {
        "id": "XegGEgBvTwwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat $HADOOP_HOME/etc/hadoop/hdfs-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXmyyxRhTzZm",
        "outputId": "9468c212-fd5e-428f-ca01-7cad491c47c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "\n",
            "<!-- Put site-specific property overrides in this file. -->\n",
            "\n",
            "<configuration>\n",
            "<property>\n",
            "     <name>dfs.replication</name>\n",
            "     <value>1</value>\n",
            "   </property>\n",
            "\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding required properties to mapred-site.xml file\n",
        "!sed -i '/<configuration>/a\\\n",
        "  <property>\\n\\\n",
        "    <name>mapreduce.framework.name</name>\\n\\\n",
        "    <value>yarn</value>\\n\\\n",
        "  </property>\\n\\\n",
        "  <property>\\n\\\n",
        "    <name>mapreduce.application.classpath</name>\\n\\\n",
        "    <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>\\n\\\n",
        "  </property>' \\\n",
        "$HADOOP_HOME/etc/hadoop/mapred-site.xml"
      ],
      "metadata": {
        "id": "CfkkXPijR-NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat $HADOOP_HOME/etc/hadoop/mapred-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNN17Kg8UA2j",
        "outputId": "942b6034-5b7e-4ce5-e80c-7738cdc5cdf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "\n",
            "<!-- Put site-specific property overrides in this file. -->\n",
            "\n",
            "<configuration>\n",
            "<property>\n",
            "     <name>mapreduce.framework.name</name>\n",
            "     <value>yarn</value>\n",
            "   </property>\n",
            "   <property>\n",
            "     <name>mapreduce.application.classpath</name>\n",
            "     <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>\n",
            "   </property>\n",
            "\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding required properties to yarn-site.xml file\n",
        "!sed -i '/<configuration>/a\\\n",
        "  <property>\\n\\\n",
        "    <description>The hostname of the RM.</description>\\n\\\n",
        "    <name>yarn.resourcemanager.hostname</name>\\n\\\n",
        "    <value>localhost</value>\\n\\\n",
        "  </property>\\n\\\n",
        "  <property>\\n\\\n",
        "    <name>yarn.nodemanager.aux-services</name>\\n\\\n",
        "    <value>mapreduce_shuffle</value>\\n\\\n",
        "  </property>\\n\\\n",
        "  <property>\\n\\\n",
        "    <name>yarn.nodemanager.env-whitelist</name>\\n\\\n",
        "    <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME</value>\\n\\\n",
        "  </property>' \\\n",
        "$HADOOP_HOME/etc/hadoop/yarn-site.xml"
      ],
      "metadata": {
        "id": "LdGCJEBOUFwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Content of yarn-site.xml after the editing\n",
        "!cat $HADOOP_HOME/etc/hadoop/yarn-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rt8s4GxSR_Eh",
        "outputId": "f6f32663-6abc-4804-cd3b-afdb31fcdf4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "<configuration>\n",
            "<property>\n",
            "     <description>The hostname of the RM.</description>\n",
            "     <name>yarn.resourcemanager.hostname</name>\n",
            "     <value>localhost</value>\n",
            "   </property>\n",
            "   <property>\n",
            "     <name>yarn.nodemanager.aux-services</name>\n",
            "     <value>mapreduce_shuffle</value>\n",
            "   </property>\n",
            "   <property>\n",
            "     <name>yarn.nodemanager.env-whitelist</name>\n",
            "     <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME</value>\n",
            "   </property>\n",
            "\n",
            "<!-- Site specific YARN configuration properties -->\n",
            "\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Formateo del sistema de archivos HDFS\n",
        "\n",
        "Antes de poder utilizar HDFS por primera vez, se debe formatear el sistema de archivos. El proceso de formateo crea un sistema de archivos vacío al crear los directorios de almacenamiento y las versiones iniciales de los NameNodes."
      ],
      "metadata": {
        "id": "D-_T0kPiSPQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs namenode -format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wNQPizCSBk8",
        "outputId": "218c6ffd-24b6-4e0c-a714-71282aee6c2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: /usr/local/hadoop-3.2.4/logs does not exist. Creating.\n",
            "2023-10-16 14:44:05,777 INFO namenode.NameNode: STARTUP_MSG: \n",
            "/************************************************************\n",
            "STARTUP_MSG: Starting NameNode\n",
            "STARTUP_MSG:   host = b12352b94bc7/172.28.0.12\n",
            "STARTUP_MSG:   args = [-format]\n",
            "STARTUP_MSG:   version = 3.2.4\n",
            "STARTUP_MSG:   classpath = /usr/local/hadoop-3.2.4/etc/hadoop:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/accessors-smart-2.4.7.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jackson-databind-2.10.5.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/reload4j-1.2.18.3.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jetty-util-9.4.43.v20210629.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jetty-security-9.4.43.v20210629.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jetty-servlet-9.4.43.v20210629.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jetty-xml-9.4.43.v20210629.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/commons-lang3-3.7.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/netty-3.10.6.Final.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/slf4j-reload4j-1.7.35.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jetty-util-ajax-9.4.43.v20210629.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/slf4j-api-1.7.35.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/asm-5.0.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/commons-io-2.8.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/spotbugs-annotations-3.1.9.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/commons-compress-1.21.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/commons-net-3.6.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/javax.activation-api-1.2.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/curator-client-2.13.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jetty-server-9.4.43.v20210629.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jetty-webapp-9.4.43.v20210629.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/hadoop-auth-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/woodstox-core-5.3.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/commons-codec-1.11.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/commons-text-1.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jackson-core-2.10.5.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/json-smart-2.4.7.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/curator-framework-2.13.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/hadoop-annotations-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/zookeeper-3.4.14.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jetty-http-9.4.43.v20210629.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jul-to-slf4j-1.7.35.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/gson-2.9.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jetty-io-9.4.43.v20210629.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jackson-annotations-2.10.5.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/hadoop-common-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/hadoop-common-3.2.4-tests.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/hadoop-kms-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/common/hadoop-nfs-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/accessors-smart-2.4.7.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jackson-databind-2.10.5.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/reload4j-1.2.18.3.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jetty-util-9.4.43.v20210629.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jetty-security-9.4.43.v20210629.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jetty-servlet-9.4.43.v20210629.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jetty-xml-9.4.43.v20210629.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/okio-1.6.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.43.v20210629.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/spotbugs-annotations-3.1.9.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/commons-net-3.6.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/javax.activation-api-1.2.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jetty-server-9.4.43.v20210629.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jetty-webapp-9.4.43.v20210629.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/hadoop-auth-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/woodstox-core-5.3.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/commons-text-1.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/netty-all-4.1.68.Final.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jackson-core-2.10.5.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/json-smart-2.4.7.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/hadoop-annotations-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/zookeeper-3.4.14.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jetty-http-9.4.43.v20210629.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/gson-2.9.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jetty-io-9.4.43.v20210629.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jackson-annotations-2.10.5.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/hadoop-hdfs-client-3.2.4-tests.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.4-tests.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/hadoop-hdfs-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.4-tests.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/hadoop-hdfs-3.2.4-tests.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/hadoop-hdfs-client-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-3.2.4/share/hadoop/mapreduce/lib/junit-4.13.2.jar:/usr/local/hadoop-3.2.4/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.4-tests.jar:/usr/local/hadoop-3.2.4/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn:/usr/local/hadoop-3.2.4/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/lib/guice-4.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/lib/snakeyaml-1.26.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/lib/objenesis-1.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/hadoop-yarn-common-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/hadoop-yarn-api-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/hadoop-yarn-client-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/hadoop-yarn-submarine-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/hadoop-yarn-services-core-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/hadoop-yarn-registry-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/hadoop-yarn-server-router-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/hadoop-yarn-server-common-3.2.4.jar:/usr/local/hadoop-3.2.4/share/hadoop/yarn/hadoop-yarn-services-api-3.2.4.jar\n",
            "STARTUP_MSG:   build = Unknown -r 7e5d9983b388e372fe640f21f048f2f2ae6e9eba; compiled by 'ubuntu' on 2022-07-12T11:58Z\n",
            "STARTUP_MSG:   java = 1.8.0_382\n",
            "************************************************************/\n",
            "2023-10-16 14:44:05,795 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
            "2023-10-16 14:44:05,942 INFO namenode.NameNode: createNameNode [-format]\n",
            "Formatting using clusterid: CID-d3bd0855-77ed-4b67-aecf-2dcdfd7d98cc\n",
            "2023-10-16 14:44:06,878 INFO namenode.FSEditLog: Edit logging is async:true\n",
            "2023-10-16 14:44:06,915 INFO namenode.FSNamesystem: KeyProvider: null\n",
            "2023-10-16 14:44:06,917 INFO namenode.FSNamesystem: fsLock is fair: true\n",
            "2023-10-16 14:44:06,918 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
            "2023-10-16 14:44:06,926 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\n",
            "2023-10-16 14:44:06,926 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
            "2023-10-16 14:44:06,926 INFO namenode.FSNamesystem: isPermissionEnabled = true\n",
            "2023-10-16 14:44:06,926 INFO namenode.FSNamesystem: HA Enabled: false\n",
            "2023-10-16 14:44:06,980 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
            "2023-10-16 14:44:06,992 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n",
            "2023-10-16 14:44:06,992 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
            "2023-10-16 14:44:07,020 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
            "2023-10-16 14:44:07,021 INFO blockmanagement.BlockManager: The block deletion will start around 2023 Oct 16 14:44:07\n",
            "2023-10-16 14:44:07,024 INFO util.GSet: Computing capacity for map BlocksMap\n",
            "2023-10-16 14:44:07,024 INFO util.GSet: VM type       = 64-bit\n",
            "2023-10-16 14:44:07,026 INFO util.GSet: 2.0% max memory 2.8 GB = 57.7 MB\n",
            "2023-10-16 14:44:07,026 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n",
            "2023-10-16 14:44:07,043 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
            "2023-10-16 14:44:07,043 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
            "2023-10-16 14:44:07,050 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
            "2023-10-16 14:44:07,050 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
            "2023-10-16 14:44:07,050 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
            "2023-10-16 14:44:07,051 INFO blockmanagement.BlockManager: defaultReplication         = 1\n",
            "2023-10-16 14:44:07,051 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
            "2023-10-16 14:44:07,051 INFO blockmanagement.BlockManager: minReplication             = 1\n",
            "2023-10-16 14:44:07,051 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
            "2023-10-16 14:44:07,051 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
            "2023-10-16 14:44:07,051 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
            "2023-10-16 14:44:07,051 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
            "2023-10-16 14:44:07,078 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
            "2023-10-16 14:44:07,078 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
            "2023-10-16 14:44:07,078 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
            "2023-10-16 14:44:07,078 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
            "2023-10-16 14:44:07,095 INFO util.GSet: Computing capacity for map INodeMap\n",
            "2023-10-16 14:44:07,095 INFO util.GSet: VM type       = 64-bit\n",
            "2023-10-16 14:44:07,095 INFO util.GSet: 1.0% max memory 2.8 GB = 28.9 MB\n",
            "2023-10-16 14:44:07,095 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
            "2023-10-16 14:44:07,102 INFO namenode.FSDirectory: ACLs enabled? false\n",
            "2023-10-16 14:44:07,102 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
            "2023-10-16 14:44:07,102 INFO namenode.FSDirectory: XAttrs enabled? true\n",
            "2023-10-16 14:44:07,103 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
            "2023-10-16 14:44:07,111 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
            "2023-10-16 14:44:07,114 INFO snapshot.SnapshotManager: SkipList is disabled\n",
            "2023-10-16 14:44:07,119 INFO util.GSet: Computing capacity for map cachedBlocks\n",
            "2023-10-16 14:44:07,119 INFO util.GSet: VM type       = 64-bit\n",
            "2023-10-16 14:44:07,120 INFO util.GSet: 0.25% max memory 2.8 GB = 7.2 MB\n",
            "2023-10-16 14:44:07,120 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
            "2023-10-16 14:44:07,133 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
            "2023-10-16 14:44:07,133 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
            "2023-10-16 14:44:07,133 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
            "2023-10-16 14:44:07,138 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
            "2023-10-16 14:44:07,139 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
            "2023-10-16 14:44:07,141 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
            "2023-10-16 14:44:07,141 INFO util.GSet: VM type       = 64-bit\n",
            "2023-10-16 14:44:07,141 INFO util.GSet: 0.029999999329447746% max memory 2.8 GB = 886.4 KB\n",
            "2023-10-16 14:44:07,141 INFO util.GSet: capacity      = 2^17 = 131072 entries\n",
            "2023-10-16 14:44:07,181 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1388335291-172.28.0.12-1697467447169\n",
            "2023-10-16 14:44:07,208 INFO common.Storage: Storage directory /tmp/hadoop-root/dfs/name has been successfully formatted.\n",
            "2023-10-16 14:44:07,255 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n",
            "2023-10-16 14:44:07,416 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\n",
            "2023-10-16 14:44:07,447 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
            "2023-10-16 14:44:07,521 INFO namenode.FSNamesystem: Stopping services started for active state\n",
            "2023-10-16 14:44:07,521 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
            "2023-10-16 14:44:07,535 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
            "2023-10-16 14:44:07,535 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
            "/************************************************************\n",
            "SHUTDOWN_MSG: Shutting down NameNode at b12352b94bc7/172.28.0.12\n",
            "************************************************************/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring Hadoop scripts available in sbin directory\n",
        "!ls $HADOOP_HOME/sbin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HOtQrP4UVMc",
        "outputId": "44bf24f4-8e2b-4f58-a11d-a9e8b3a21e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distribute-exclude.sh\t refresh-namenodes.sh  start-yarn.cmd\t stop-secure-dns.sh\n",
            "FederationStateStore\t start-all.cmd\t       start-yarn.sh\t stop-yarn.cmd\n",
            "hadoop-daemon.sh\t start-all.sh\t       stop-all.cmd\t stop-yarn.sh\n",
            "hadoop-daemons.sh\t start-balancer.sh     stop-all.sh\t workers.sh\n",
            "httpfs.sh\t\t start-dfs.cmd\t       stop-balancer.sh  yarn-daemon.sh\n",
            "kms.sh\t\t\t start-dfs.sh\t       stop-dfs.cmd\t yarn-daemons.sh\n",
            "mr-jobhistory-daemon.sh  start-secure-dns.sh   stop-dfs.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating other necessary enviroment variables before starting nodes\n",
        "os.environ[\"HDFS_NAMENODE_USER\"] = \"root\"\n",
        "os.environ[\"HDFS_DATANODE_USER\"] = \"root\"\n",
        "os.environ[\"HDFS_SECONDARYNAMENODE_USER\"] = \"root\"\n",
        "os.environ[\"YARN_RESOURCEMANAGER_USER\"] = \"root\"\n",
        "os.environ[\"YARN_NODEMANAGER_USER\"] = \"root\""
      ],
      "metadata": {
        "id": "X9nIOwiJSIlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Launching hdfs deamon\n",
        "!$HADOOP_HOME/sbin/start-dfs.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ge9yaMGSXLA",
        "outputId": "75875d59-72a9-4e5f-a0eb-55eceb7ea8fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting namenodes on [localhost]\n",
            "Starting datanodes\n",
            "Starting secondary namenodes [b12352b94bc7]\n",
            "b12352b94bc7: Warning: Permanently added 'b12352b94bc7' (ED25519) to the list of known hosts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing the running deamons\n",
        "!jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYJDpaOWSnFD",
        "outputId": "47b5a715-c764-442e-fa51-b49066f5eb0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4119 DataNode\n",
            "4007 NameNode\n",
            "4303 SecondaryNameNode\n",
            "4479 Jps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup $HADOOP_HOME/sbin/start-yarn.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ID_asMCXU6Cr",
        "outputId": "f092de7e-ea3b-4936-c96f-2a10fcdf48d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: ignoring input and appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/sbin/stop-yarn.sh\n",
        "!$HADOOP_HOME/sbin/stop-dfs.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFeI5tPwlAIC",
        "outputId": "1613b69d-e8d3-465b-d288-e767fa0a176b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping nodemanagers\n",
            "Stopping resourcemanager\n",
            "Stopping namenodes on [localhost]\n",
            "Stopping datanodes\n",
            "Stopping secondary namenodes [3e881b068796]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing the running deamons\n",
        "!jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x09v8g6FU76T",
        "outputId": "4edbf699-ed0c-47de-8cfe-bc40ba4ccd2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4119 DataNode\n",
            "4007 NameNode\n",
            "4743 NodeManager\n",
            "4632 ResourceManager\n",
            "5115 Jps\n",
            "4303 SecondaryNameNode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Report the basic file system information and statistics\n",
        "!$HADOOP_HOME/bin/hdfs dfsadmin -report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVxRN7ZnSqCq",
        "outputId": "75a5d651-fdcf-4da1-c6c4-a1949043a954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configured Capacity: 115658190848 (107.72 GB)\n",
            "Present Capacity: 83721641984 (77.97 GB)\n",
            "DFS Remaining: 83721617408 (77.97 GB)\n",
            "DFS Used: 24576 (24 KB)\n",
            "DFS Used%: 0.00%\n",
            "Replicated Blocks:\n",
            "\tUnder replicated blocks: 0\n",
            "\tBlocks with corrupt replicas: 0\n",
            "\tMissing blocks: 0\n",
            "\tMissing blocks (with replication factor 1): 0\n",
            "\tLow redundancy blocks with highest priority to recover: 0\n",
            "\tPending deletion blocks: 0\n",
            "Erasure Coded Block Groups: \n",
            "\tLow redundancy block groups: 0\n",
            "\tBlock groups with corrupt internal blocks: 0\n",
            "\tMissing block groups: 0\n",
            "\tLow redundancy blocks with highest priority to recover: 0\n",
            "\tPending deletion blocks: 0\n",
            "\n",
            "-------------------------------------------------\n",
            "Live datanodes (1):\n",
            "\n",
            "Name: 127.0.0.1:9866 (localhost)\n",
            "Hostname: b12352b94bc7\n",
            "Decommission Status : Normal\n",
            "Configured Capacity: 115658190848 (107.72 GB)\n",
            "DFS Used: 24576 (24 KB)\n",
            "Non DFS Used: 31919771648 (29.73 GB)\n",
            "DFS Remaining: 83721617408 (77.97 GB)\n",
            "DFS Used%: 0.00%\n",
            "DFS Remaining%: 72.39%\n",
            "Configured Cache Capacity: 0 (0 B)\n",
            "Cache Used: 0 (0 B)\n",
            "Cache Remaining: 0 (0 B)\n",
            "Cache Used%: 100.00%\n",
            "Cache Remaining%: 0.00%\n",
            "Xceivers: 1\n",
            "Last contact: Mon Oct 16 14:45:33 UTC 2023\n",
            "Last Block Report: Mon Oct 16 14:44:37 UTC 2023\n",
            "Num of Blocks: 0\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supervisión del clúster de Hadoop con la interfaz del navegador"
      ],
      "metadata": {
        "id": "aCvXXsqqS8ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "#The namenode posts the general report on port 9870\n",
        "output.serve_kernel_port_as_window(9870)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Jz-KO-w6S_Fd",
        "outputId": "32bca679-a2a6-435d-be78-7e1459fb7387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(9870, \"/\", \"https://localhost:9870/\", window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejecutando el conteo de palabras en modo pseudodistribuido"
      ],
      "metadata": {
        "id": "RbVYpjSLTE4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget  https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCid70yETM04",
        "outputId": "ec67577b-2411-4220-b361-ac38892eb365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-16 14:46:15--  https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt\n",
            "Resolving www.mirrorservice.org (www.mirrorservice.org)... 212.219.56.184, 2001:630:341:12::184\n",
            "Connecting to www.mirrorservice.org (www.mirrorservice.org)|212.219.56.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 678064 (662K) [text/plain]\n",
            "Saving to: ‘101.txt.1’\n",
            "\n",
            "101.txt.1           100%[===================>] 662.17K   800KB/s    in 0.8s    \n",
            "\n",
            "2023-10-16 14:46:17 (800 KB/s) - ‘101.txt.1’ saved [678064/678064]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /word_count\n",
        "#Coping file from local file system to HDFS\n",
        "!$HADOOP_HOME/bin/hdfs dfs -put /content/101.txt /word_count"
      ],
      "metadata": {
        "id": "JVn15dxhTTRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wP3oYp54TUJp",
        "outputId": "2b4dcf6c-b570-483d-c4d9-3a61636bee4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 items\n",
            "-rw-r--r--   1 root supergroup     678064 2023-10-16 14:46 /word_count/101.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.4.jar wordcount /word_count/101.txt /word_count/output/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmXiVvQxTZT2",
        "outputId": "5cfa0a8a-fbfc-4958-bb5f-a7b8de78c71d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-16 14:46:53,077 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
            "2023-10-16 14:46:54,259 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1697467505856_0001\n",
            "2023-10-16 14:46:54,860 INFO input.FileInputFormat: Total input files to process : 1\n",
            "2023-10-16 14:46:55,490 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2023-10-16 14:46:55,720 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1697467505856_0001\n",
            "2023-10-16 14:46:55,722 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2023-10-16 14:46:56,001 INFO conf.Configuration: resource-types.xml not found\n",
            "2023-10-16 14:46:56,002 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
            "2023-10-16 14:46:56,587 INFO impl.YarnClientImpl: Submitted application application_1697467505856_0001\n",
            "2023-10-16 14:46:56,651 INFO mapreduce.Job: The url to track the job: http://b12352b94bc7:8088/proxy/application_1697467505856_0001/\n",
            "2023-10-16 14:46:56,652 INFO mapreduce.Job: Running job: job_1697467505856_0001\n",
            "2023-10-16 14:47:09,065 INFO mapreduce.Job: Job job_1697467505856_0001 running in uber mode : false\n",
            "2023-10-16 14:47:09,067 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2023-10-16 14:47:16,174 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2023-10-16 14:47:23,261 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2023-10-16 14:47:24,283 INFO mapreduce.Job: Job job_1697467505856_0001 completed successfully\n",
            "2023-10-16 14:47:24,438 INFO mapreduce.Job: Counters: 54\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=318005\n",
            "\t\tFILE: Number of bytes written=1111593\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=678169\n",
            "\t\tHDFS: Number of bytes written=233636\n",
            "\t\tHDFS: Number of read operations=8\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=2\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tJob Counters \n",
            "\t\tLaunched map tasks=1\n",
            "\t\tLaunched reduce tasks=1\n",
            "\t\tData-local map tasks=1\n",
            "\t\tTotal time spent by all maps in occupied slots (ms)=4313\n",
            "\t\tTotal time spent by all reduces in occupied slots (ms)=4585\n",
            "\t\tTotal time spent by all map tasks (ms)=4313\n",
            "\t\tTotal time spent by all reduce tasks (ms)=4585\n",
            "\t\tTotal vcore-milliseconds taken by all map tasks=4313\n",
            "\t\tTotal vcore-milliseconds taken by all reduce tasks=4585\n",
            "\t\tTotal megabyte-milliseconds taken by all map tasks=4416512\n",
            "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4695040\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=13006\n",
            "\t\tMap output records=105488\n",
            "\t\tMap output bytes=1078906\n",
            "\t\tMap output materialized bytes=318005\n",
            "\t\tInput split bytes=105\n",
            "\t\tCombine input records=105488\n",
            "\t\tCombine output records=21438\n",
            "\t\tReduce input groups=21438\n",
            "\t\tReduce shuffle bytes=318005\n",
            "\t\tReduce input records=21438\n",
            "\t\tReduce output records=21438\n",
            "\t\tSpilled Records=42876\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=241\n",
            "\t\tCPU time spent (ms)=3930\n",
            "\t\tPhysical memory (bytes) snapshot=570236928\n",
            "\t\tVirtual memory (bytes) snapshot=5089959936\n",
            "\t\tTotal committed heap usage (bytes)=615514112\n",
            "\t\tPeak Map Physical memory (bytes)=346734592\n",
            "\t\tPeak Map Virtual memory (bytes)=2546794496\n",
            "\t\tPeak Reduce Physical memory (bytes)=223502336\n",
            "\t\tPeak Reduce Virtual memory (bytes)=2543165440\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=678064\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=233636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count/output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rs2reQR5VHiE",
        "outputId": "f84f9957-b0ab-4532-ebba-a4c719c29631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root supergroup          0 2023-10-16 14:47 /word_count/output/_SUCCESS\n",
            "-rw-r--r--   1 root supergroup     233636 2023-10-16 14:47 /word_count/output/part-r-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -cat /word_count/output/part-r-00000 | sort -rn -k2,2 |head -10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s6AnVTAVKIQ",
        "outputId": "7acab759-f469-43ac-c157-627350d11789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the\t5100\n",
            "of\t3356\n",
            "and\t2765\n",
            "a\t2434\n",
            "to\t2243\n",
            "in\t1693\n",
            "was\t1096\n",
            "is\t829\n",
            "for\t781\n",
            "that\t771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring Hadoop utilities available\n",
        "!ls $HADOOP_HOME/share/hadoop/tools/lib/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gg5voVtXVVfl",
        "outputId": "c216f821-8a8e-467b-e4e4-ecbce0cee9bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aliyun-java-sdk-core-4.5.10.jar      hadoop-gridmix-3.2.4.jar\n",
            "aliyun-java-sdk-kms-2.11.0.jar\t     hadoop-kafka-3.2.4.jar\n",
            "aliyun-java-sdk-ram-3.1.0.jar\t     hadoop-openstack-3.2.4.jar\n",
            "aliyun-sdk-oss-3.13.0.jar\t     hadoop-resourceestimator-3.2.4.jar\n",
            "aws-java-sdk-bundle-1.11.901.jar     hadoop-rumen-3.2.4.jar\n",
            "azure-data-lake-store-sdk-2.2.9.jar  hadoop-sls-3.2.4.jar\n",
            "azure-keyvault-core-1.0.0.jar\t     hadoop-streaming-3.2.4.jar\n",
            "azure-storage-7.0.0.jar\t\t     ini4j-0.5.4.jar\n",
            "hadoop-aliyun-3.2.4.jar\t\t     jdom2-2.0.6.jar\n",
            "hadoop-archive-logs-3.2.4.jar\t     kafka-clients-2.8.1.jar\n",
            "hadoop-archives-3.2.4.jar\t     lz4-java-1.7.1.jar\n",
            "hadoop-aws-3.2.4.jar\t\t     ojalgo-43.0.jar\n",
            "hadoop-azure-3.2.4.jar\t\t     opentracing-api-0.33.0.jar\n",
            "hadoop-azure-datalake-3.2.4.jar      opentracing-noop-0.33.0.jar\n",
            "hadoop-datajoin-3.2.4.jar\t     opentracing-util-0.33.0.jar\n",
            "hadoop-distcp-3.2.4.jar\t\t     org.jacoco.agent-0.8.5-runtime.jar\n",
            "hadoop-extras-3.2.4.jar\t\t     wildfly-openssl-1.0.7.Final.jar\n",
            "hadoop-fs2img-3.2.4.jar\t\t     zstd-jni-1.4.9-1.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /word_count_with_python\n",
        "!$HADOOP_HOME/bin/hdfs dfs -put /content/101.txt /word_count_with_python"
      ],
      "metadata": {
        "id": "6UwyQ7mPVZFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Programming mapper and reducer before Hadoop"
      ],
      "metadata": {
        "id": "vwDiunx45K5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# contar palabras en archivo de texto"
      ],
      "metadata": {
        "id": "N34R8Pxc5cs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt"
      ],
      "metadata": {
        "id": "k3qaezS15TOy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! head 101.txt"
      ],
      "metadata": {
        "id": "TIAakwTn5hDG",
        "outputId": "0a98d3b7-e6fd-4709-95a6-c59ba9b3c91b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Project Gutenberg EBook of Hacker Crackdown, by Bruce Sterling\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere at no cost and with\r\n",
            "almost no restrictions whatsoever.  You may copy it, give it away or\r\n",
            "re-use it under the terms of the Project Gutenberg License included\r\n",
            "with this eBook or online at www.gutenberg.org\r\n",
            "\r\n",
            "** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **\r\n",
            "**     Please follow the copyright guidelines in this file.     **\r\n",
            "\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "# import sys because we need to read and write data to STDIN and STDOUT\n",
        "import sys\n",
        "\n",
        "# reading entire line from STDIN (standard input)\n",
        "for line in sys.stdin:\n",
        "  line = line.strip()\n",
        "  words = line.split()\n",
        "  for word in words:\n",
        "    print('%s\\t%s' % (word, 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtQt8AM_VgnS",
        "outputId": "8ff94613-bf23-4851-bc7d-eaead9c9c5f8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cat 101.txt | python3 mapper.py | sort -k1,1 | head"
      ],
      "metadata": {
        "id": "Eq7GuJCo8vhG",
        "outputId": "a43a0124-fd82-4223-f9e0-5324fce9f638",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#\t1\n",
            "#\t1\n",
            "#\t1\n",
            "#\t1\n",
            "#\t1\n",
            "#\t1\n",
            "#\t1\n",
            "#\t1\n",
            "#\t1\n",
            "#\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "from operator import itemgetter\n",
        "import sys\n",
        "\n",
        "current_word = None\n",
        "current_count = 0\n",
        "word = None\n",
        "\n",
        "# read the entire line from STDIN\n",
        "for line in sys.stdin:\n",
        "  # remove leading and trailing whitespace\n",
        "  line = line.strip()\n",
        "  # splitting the data on the basis of tab we have provided in mapper.py\n",
        "  word, count = line.split('\\t', 1)\n",
        "  # convert count (currently a string) to int\n",
        "  try:\n",
        "    count = int(count)\n",
        "  except ValueError:\n",
        "    # count was not a number, so silently\n",
        "    # ignore/discard this line\n",
        "    continue\n",
        "\n",
        "  # this IF-switch only works because Hadoop sorts map output\n",
        "  # by key (here: word) before it is passed to the reducer\n",
        "  if current_word == word:\n",
        "    current_count += count\n",
        "  else:\n",
        "    if current_word: #to not print current_word=None\n",
        "      # write result to STDOUT\n",
        "      print('%s\\t%s' % (current_word, current_count))\n",
        "    current_count = count\n",
        "    current_word = word\n",
        "\n",
        "# do not forget to output the last word if needed!\n",
        "if current_word == word:\n",
        "  print('%s\\t%s' % (current_word, current_count))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqBMkeeGVt-v",
        "outputId": "611cc80b-1290-41e2-ce0a-63b88b65e11c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reducer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat 101.txt | python mapper.py | sort -k1,1 |python reducer.py   | sort -nr -k2,2 | head -n 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJnkx6B3VzDS",
        "outputId": "7d184fc9-59e3-4b67-85ec-28c3f552ac3e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the\t5100\n",
            "of\t3356\n",
            "and\t2765\n",
            "a\t2434\n",
            "to\t2243\n",
            "in\t1693\n",
            "was\t1096\n",
            "is\t829\n",
            "for\t781\n",
            "that\t771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 777 /content/mapper.py /content/reducer.py"
      ],
      "metadata": {
        "id": "xe3NHB1nWBGw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running MapReduce programs\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.4.jar \\\n",
        "  -input /word_count_with_python/101.txt \\\n",
        "  -output /word_count_with_python/output \\\n",
        "  -mapper \"python /content/mapper.py\" \\\n",
        "  -reducer \"python /content/reducer.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr6JX-viWFDo",
        "outputId": "467d13a1-546a-47df-9fdc-77d1d66aefb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "packageJobJar: [/tmp/hadoop-unjar128140512875733733/] [] /tmp/streamjob2246386237191257575.jar tmpDir=null\n",
            "2023-10-16 14:50:07,406 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
            "2023-10-16 14:50:07,718 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
            "2023-10-16 14:50:08,058 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1697467505856_0002\n",
            "2023-10-16 14:50:08,429 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2023-10-16 14:50:08,555 INFO mapreduce.JobSubmitter: number of splits:2\n",
            "2023-10-16 14:50:08,773 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1697467505856_0002\n",
            "2023-10-16 14:50:08,775 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2023-10-16 14:50:09,038 INFO conf.Configuration: resource-types.xml not found\n",
            "2023-10-16 14:50:09,040 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
            "2023-10-16 14:50:09,125 INFO impl.YarnClientImpl: Submitted application application_1697467505856_0002\n",
            "2023-10-16 14:50:09,189 INFO mapreduce.Job: The url to track the job: http://b12352b94bc7:8088/proxy/application_1697467505856_0002/\n",
            "2023-10-16 14:50:09,192 INFO mapreduce.Job: Running job: job_1697467505856_0002\n",
            "2023-10-16 14:50:18,412 INFO mapreduce.Job: Job job_1697467505856_0002 running in uber mode : false\n",
            "2023-10-16 14:50:18,413 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2023-10-16 14:50:28,618 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2023-10-16 14:50:34,701 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2023-10-16 14:50:35,717 INFO mapreduce.Job: Job job_1697467505856_0002 completed successfully\n",
            "2023-10-16 14:50:35,833 INFO mapreduce.Job: Counters: 55\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1078912\n",
            "\t\tFILE: Number of bytes written=2877574\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=682368\n",
            "\t\tHDFS: Number of bytes written=233636\n",
            "\t\tHDFS: Number of read operations=11\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=2\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tJob Counters \n",
            "\t\tKilled map tasks=1\n",
            "\t\tLaunched map tasks=2\n",
            "\t\tLaunched reduce tasks=1\n",
            "\t\tData-local map tasks=2\n",
            "\t\tTotal time spent by all maps in occupied slots (ms)=16746\n",
            "\t\tTotal time spent by all reduces in occupied slots (ms)=3784\n",
            "\t\tTotal time spent by all map tasks (ms)=16746\n",
            "\t\tTotal time spent by all reduce tasks (ms)=3784\n",
            "\t\tTotal vcore-milliseconds taken by all map tasks=16746\n",
            "\t\tTotal vcore-milliseconds taken by all reduce tasks=3784\n",
            "\t\tTotal megabyte-milliseconds taken by all map tasks=17147904\n",
            "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3874816\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=13006\n",
            "\t\tMap output records=105488\n",
            "\t\tMap output bytes=867930\n",
            "\t\tMap output materialized bytes=1078918\n",
            "\t\tInput split bytes=208\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=21438\n",
            "\t\tReduce shuffle bytes=1078918\n",
            "\t\tReduce input records=105488\n",
            "\t\tReduce output records=21438\n",
            "\t\tSpilled Records=210976\n",
            "\t\tShuffled Maps =2\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=2\n",
            "\t\tGC time elapsed (ms)=454\n",
            "\t\tCPU time spent (ms)=4990\n",
            "\t\tPhysical memory (bytes) snapshot=831197184\n",
            "\t\tVirtual memory (bytes) snapshot=7617110016\n",
            "\t\tTotal committed heap usage (bytes)=898105344\n",
            "\t\tPeak Map Physical memory (bytes)=326139904\n",
            "\t\tPeak Map Virtual memory (bytes)=2539388928\n",
            "\t\tPeak Reduce Physical memory (bytes)=222171136\n",
            "\t\tPeak Reduce Virtual memory (bytes)=2541449216\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=682160\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=233636\n",
            "2023-10-16 14:50:35,833 INFO streaming.StreamJob: Output directory: /word_count_with_python/output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -cat /word_count_with_python/output/part-00000 | sort -rn -k2,2 | head -10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgQm9w9uWNc4",
        "outputId": "e4bb833c-9d5b-4cb4-bb22-4347f001f594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the\t5100\n",
            "of\t3356\n",
            "and\t2765\n",
            "a\t2434\n",
            "to\t2243\n",
            "in\t1693\n",
            "was\t1096\n",
            "is\t829\n",
            "for\t781\n",
            "that\t771\n"
          ]
        }
      ]
    }
  ]
}