{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNC1NJOictUUj0ZulvTP/oB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adigenova/uohpmd/blob/main/code/MPI_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install mpich"
      ],
      "metadata": {
        "id": "y1rCHrYWAHl9",
        "outputId": "9beb02e3-3e00-4914-ccdb-08fef2b7bbb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  hwloc-nox libmpich-dev libmpich12 libslurm37\n",
            "Suggested packages:\n",
            "  mpich-doc\n",
            "The following NEW packages will be installed:\n",
            "  hwloc-nox libmpich-dev libmpich12 libslurm37 mpich\n",
            "0 upgraded, 5 newly installed, 0 to remove and 18 not upgraded.\n",
            "Need to get 14.2 MB of archives.\n",
            "After this operation, 102 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libslurm37 amd64 21.08.5-2ubuntu1 [542 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 hwloc-nox amd64 2.7.0-2ubuntu1 [205 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmpich12 amd64 4.0-3 [5,866 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 mpich amd64 4.0-3 [197 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmpich-dev amd64 4.0-3 [7,375 kB]\n",
            "Fetched 14.2 MB in 1s (25.5 MB/s)\n",
            "Selecting previously unselected package libslurm37.\n",
            "(Reading database ... 120895 files and directories currently installed.)\n",
            "Preparing to unpack .../libslurm37_21.08.5-2ubuntu1_amd64.deb ...\n",
            "Unpacking libslurm37 (21.08.5-2ubuntu1) ...\n",
            "Selecting previously unselected package hwloc-nox.\n",
            "Preparing to unpack .../hwloc-nox_2.7.0-2ubuntu1_amd64.deb ...\n",
            "Unpacking hwloc-nox (2.7.0-2ubuntu1) ...\n",
            "Selecting previously unselected package libmpich12:amd64.\n",
            "Preparing to unpack .../libmpich12_4.0-3_amd64.deb ...\n",
            "Unpacking libmpich12:amd64 (4.0-3) ...\n",
            "Selecting previously unselected package mpich.\n",
            "Preparing to unpack .../archives/mpich_4.0-3_amd64.deb ...\n",
            "Unpacking mpich (4.0-3) ...\n",
            "Selecting previously unselected package libmpich-dev:amd64.\n",
            "Preparing to unpack .../libmpich-dev_4.0-3_amd64.deb ...\n",
            "Unpacking libmpich-dev:amd64 (4.0-3) ...\n",
            "Setting up libslurm37 (21.08.5-2ubuntu1) ...\n",
            "Setting up hwloc-nox (2.7.0-2ubuntu1) ...\n",
            "Setting up libmpich12:amd64 (4.0-3) ...\n",
            "Setting up mpich (4.0-3) ...\n",
            "Setting up libmpich-dev:amd64 (4.0-3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mpi_sum.c\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <mpi.h>\n",
        "\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int rank, size;\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    int N = 1000; // Tamaño del arreglo\n",
        "    int local_N = N / size; // Tamaño del subarreglo para cada proceso\n",
        "    int local_sum = 0;\n",
        "    int global_sum = 0;\n",
        "\n",
        "    int* data = NULL;\n",
        "    int* local_data = (int*)malloc(local_N * sizeof(int));\n",
        "\n",
        "    // Inicializar el arreglo de datos en el proceso 0\n",
        "    if (rank == 0) {\n",
        "        data = (int*)malloc(N * sizeof(int));\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            data[i] = i;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Distribuir los datos entre los procesos\n",
        "    MPI_Scatter(data, local_N, MPI_INT, local_data, local_N, MPI_INT, 0, MPI_COMM_WORLD);\n",
        "\n",
        "    // Calcular la suma local\n",
        "    for (int i = 0; i < local_N; i++) {\n",
        "        local_sum += local_data[i];\n",
        "    }\n",
        "    printf(\"La suma local es: %d proceso=%d\\n\", local_sum,rank);\n",
        "\n",
        "    // Sumar las sumas locales para obtener la suma global\n",
        "    MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n",
        "\n",
        "    // El proceso 0 imprime el resultado\n",
        "    if (rank == 0) {\n",
        "        printf(\"La suma global es: %d\\n\", global_sum);\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "zE_44ShcBExF",
        "outputId": "e8e9e2c6-96ab-4d85-9ce4-e85fe40b8bf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mpi_sum.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mpicc -o mpi_sum mpi_sum.c"
      ],
      "metadata": {
        "id": "V2tomc9HBfDW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mpirun --oversubscribe --allow-run-as-root -np 10 ./mpi_sum"
      ],
      "metadata": {
        "id": "8ZA0Wh3bBs0j",
        "outputId": "e5f155ac-41df-4cba-f62d-889c721fd19d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La suma local es: 24950 proceso=2\n",
            "La suma local es: 34950 proceso=3\n",
            "La suma local es: 14950 proceso=1\n",
            "La suma local es: 4950 proceso=0\n",
            "La suma local es: 74950 proceso=7\n",
            "La suma local es: 54950 proceso=5\n",
            "La suma local es: 44950 proceso=4\n",
            "La suma local es: 64950 proceso=6\n",
            "La suma local es: 84950 proceso=8\n",
            "La suma local es: 94950 proceso=9\n",
            "La suma global es: 499500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIF1GL5Y_ue7",
        "outputId": "f3af3167-4c48-4a0e-9142-748411858d8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mpi_pi.c\n"
          ]
        }
      ],
      "source": [
        "%%writefile mpi_pi.c\n",
        "\n",
        "// MPI for calculating PI\n",
        "\n",
        "#include <stdio.h>\n",
        "#include \"mpi.h\"\n",
        "#include <math.h>\n",
        "\n",
        "int main(int argc, char *argv[])\n",
        "{\n",
        "  int n, myid, numprocs, i;\n",
        "  double PI25DT = 3.141592653589793238462643;\n",
        "  double mypi, pi=0, h, sum, x;\n",
        "  MPI_Init(NULL,NULL);\n",
        "  MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n",
        "  MPI_Comm_rank(MPI_COMM_WORLD, &myid);\n",
        "\n",
        "  if(myid==0){\n",
        "    n=10000;\n",
        "  }\n",
        "  //comunicamos el numero de intervalos a cada proceso del mundo\n",
        "  //calculamos los intervalos y repetimos si es necesario\n",
        "     printf(\"my id : %d  mypi : %.16f, n=%d\\n\", myid,mypi, n);\n",
        "    MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n",
        "      printf(\"after my id : %d  mypi : %.16f, n=%d\\n\", myid,mypi, n);\n",
        "      h   = 1.0 / (double) n;\n",
        "      sum = 0.0;\n",
        "      for (i = myid + 1; i <= n; i += numprocs) {\n",
        "            x = h * ((double)i - 0.5);\n",
        "            sum += (4.0 / (1.0 + x*x));\n",
        "      }\n",
        "      mypi = h * sum;\n",
        "      printf(\"my id : %d  mypi : %.16f, n=%d\\n\", myid,mypi, n);\n",
        "\n",
        "      //colectamos los calculos de cada trabajador usando MPI_Reduce\n",
        "\n",
        "      MPI_Reduce(&mypi, &pi, 1, MPI_DOUBLE, MPI_SUM, 3,\n",
        "                MPI_COMM_WORLD);\n",
        "      //imprimimos el valor luego de sumar\n",
        "      if (myid == 3)\n",
        "          printf(\"pi es aproximadamente %.16f, El error es %.16f\\n\",\n",
        "                pi, fabs(pi - PI25DT));\n",
        "\n",
        "  MPI_Finalize();\n",
        "  return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mpicc -o mpi_pi mpi_pi.c"
      ],
      "metadata": {
        "id": "ZglNe_KXAapq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mpirun --allow-run-as-root -np 4 ./mpi_pi"
      ],
      "metadata": {
        "id": "0UE2yxYyAr8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix multiplication"
      ],
      "metadata": {
        "id": "hYYrWYdLO67n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mpi_matrix_mult.c\n",
        "\n",
        "/*\n",
        " * Un programa simple para multiplicar matrices\n",
        " * (Matrix_A  X  Matrix_B) => Matrix_C\n",
        " */\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <mpi.h>\n",
        "#include <time.h>\n",
        "\n",
        "\n",
        "#define ARRAY_SIZE 10\n",
        "\n",
        "typedef int matrix_t[ARRAY_SIZE][ARRAY_SIZE];\n",
        "\n",
        "matrix_t MA,MB,MC;\n",
        "\n",
        "/*\n",
        "Rutina para multiplicar una fila por una columna y colocar un elemento en\n",
        "matriz resultante.\n",
        "*/\n",
        "void mult(int size,\n",
        "\t  int row,\n",
        "\t  int column,\n",
        "    int rowl,\n",
        "\t  matrix_t MA,\n",
        "\t  matrix_t MB,\n",
        "\t  int *MC)\n",
        "{\n",
        "  int position;\n",
        "  //int row_l=0;\n",
        "  MC[rowl]= 0;\n",
        "  for(position = 0; position < size; position++) {\n",
        "     MC[rowl] = MC[rowl] +\n",
        "     ( MA[row][position]  *  MB[position][column] ) ;\n",
        "\n",
        "  }\n",
        "}\n",
        "\n",
        "//colocamos valores random 1-10 en las matrices\n",
        "void inicializamos_matriz(int size,\n",
        "                    matrix_t MX)\n",
        "{\n",
        "    int   row, column;\n",
        "    srand(time(0));\n",
        "    for(row = 0; row < size; row++) {\n",
        "    for (column = 0; column < size; column++) {\n",
        "      MX[row][column]=rand()%10;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void imprimir_matriz(int size,\n",
        "                    matrix_t MX)\n",
        "{\n",
        "    int   row, column;\n",
        "    for(row = 0; row < size; row ++) {\n",
        "    for (column = 0; column < size; column++) {\n",
        "      printf(\"%5d \",MX[row][column]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "  }\n",
        "}\n",
        "\n",
        "void imprimir_matriz2(int sizer,int sizec,\n",
        "                    int *MX)\n",
        "{\n",
        "    int   row, column,i=0;\n",
        "    for(row = 0; row < sizer; row++) {\n",
        "    for (column = 0; column < sizec; column++) {\n",
        "      printf(\"%5d \",MX[i]);\n",
        "      i++;\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "// inicializamos valores y calcula los resultados\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int      size, row, column;\n",
        "\n",
        "  size = ARRAY_SIZE;\n",
        "\n",
        "//puntero a la matriz resultante\n",
        "int *final_matrix;\n",
        "int num_worker, rank;\n",
        "MPI_Init(NULL, NULL);\n",
        "MPI_Comm_size(MPI_COMM_WORLD, &num_worker);\n",
        "MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "\n",
        "if(rank == 0){\n",
        "  // inicializamos los valores de la MA\n",
        "  inicializamos_matriz(size, MA);\n",
        "  //inicializamos los valores de la MB\n",
        "  inicializamos_matriz(size, MB);\n",
        "  //imprimimos\n",
        "  printf(\"La matriz A es;\\n\");\n",
        "  imprimir_matriz(size,MA);\n",
        "  printf(\"La matriz B es;\\n\");\n",
        "  imprimir_matriz(size,MB);\n",
        "  //reservamos la memoria para la matriz final\n",
        "    final_matrix = (int *) malloc(sizeof(int*) * size*size);\n",
        "}\n",
        "\n",
        "\n",
        "MPI_Bcast(MA, size*size , MPI_INT, 0, MPI_COMM_WORLD);\n",
        "MPI_Bcast(MB, size*size , MPI_INT, 0, MPI_COMM_WORLD);\n",
        "\n",
        "//chequeamos si proceso 1 recibio la información\n",
        "if(rank == 1){\n",
        "  printf(\"id:%d La matriz A es;\\n\",rank);\n",
        "  imprimir_matriz(size,MA);\n",
        "  printf(\"id:%d La matriz B es;\\n\",rank);\n",
        "  imprimir_matriz(size,MB);\n",
        "}\n",
        "\n",
        "// determinamos la fila de inicio y fin para la proceso trabajador\n",
        "int startrow = rank * ( size / num_worker);\n",
        "int endrow = ((rank + 1) * ( size / num_worker)) -1;\n",
        "//calculamos las sub-matrices\n",
        "int number_of_rows = size / num_worker;\n",
        "int *result_matrix = (int *) malloc(sizeof(int*) * number_of_rows * size);\n",
        "    //multiplicamos\n",
        "    int rowl=0;\n",
        "    for(row = startrow; row <= endrow; row++) {\n",
        "     for (column = 0; column < size; column++) {\n",
        "      mult(size, row, column,rowl, MA, MB, result_matrix);\n",
        "      rowl++;\n",
        "      }\n",
        "    }\n",
        "   // log information\n",
        "   if(rank==1){\n",
        "      printf(\"id:%d  startrow=%d, endrow=%d,work=%d;\\n\",rank,startrow,endrow,number_of_rows*size);\n",
        "      imprimir_matriz2(number_of_rows,size,result_matrix);\n",
        "   }\n",
        "\n",
        "\n",
        "//recolectamos los resutlados de la matriz\n",
        "    MPI_Gather(result_matrix, number_of_rows*size, MPI_INT,\n",
        "           final_matrix, number_of_rows*size,  MPI_INT, 0, MPI_COMM_WORLD);\n",
        "\n",
        "  //imprimimos la matriz luego de recolectar los resultados\n",
        "  if(rank == 0){\n",
        "  printf(\"La matriz resultante C es (MPI);\\n\");\n",
        "  imprimir_matriz2(size,size,final_matrix);\n",
        "  }\n",
        "\n",
        " MPI_Finalize();\n",
        "\n",
        "  return 0;\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDrhLK1i7ixU",
        "outputId": "15133ef6-389d-4535-fdca-b3d8c9b36795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mpi_matrix_mult.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mpicc -o mpi_matrix_mult mpi_matrix_mult.c"
      ],
      "metadata": {
        "id": "R4q4sJO_Rcgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mpirun --allow-run-as-root -np 5 ./mpi_matrix_mult"
      ],
      "metadata": {
        "id": "2GF_bHjmRe6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enviado estructuras de datos"
      ],
      "metadata": {
        "id": "Z86dbnsGTvRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mpi_send_struct.c\n",
        "\n",
        "/*\n",
        " * enviado una strucutura\n",
        " */\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <mpi.h>\n",
        "#include <time.h>\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    int ProcRank, ProcNum, mTag = 0;\n",
        "    struct { int x;\n",
        "        int y;\n",
        "        int z;\n",
        "        } point;\n",
        "    MPI_Datatype ptype;\n",
        "    MPI_Status status;\n",
        "    MPI_Init(NULL, NULL);\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &ProcRank);\n",
        "    MPI_Type_contiguous(3, MPI_INT, &ptype);\n",
        "    MPI_Type_commit(&ptype);\n",
        "    if (ProcRank == 1) {\n",
        "        point.x = 45; point.y = 36; point.z = 0;\n",
        "        MPI_Send(&point, 1, ptype, 0, mTag, MPI_COMM_WORLD);\n",
        "    }\n",
        "    if (ProcRank == 0) {\n",
        "        MPI_Recv(&point, 1, ptype, 1, mTag, MPI_COMM_WORLD, &status);\n",
        "        printf(\"Proceso: %d recibio punto con  coordenadas (%d; %d; %d)\\n\", ProcRank, point.x, point.y, point.z);\n",
        "    }\n",
        "    MPI_Finalize();\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9BXMPgHSeiF",
        "outputId": "86ced78d-007b-447c-dc73-865b5bce2e46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mpi_send_struct.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mpicc -o mpi_send_struct mpi_send_struct.c"
      ],
      "metadata": {
        "id": "hHPcQ_n7S-PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mpirun --allow-run-as-root -np 2 ./mpi_send_struct"
      ],
      "metadata": {
        "id": "BPx_3rSTTDk7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}